{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5475d9f7",
   "metadata": {},
   "source": [
    "#  An√°lise de Dados: H√°bitos e Desempenho Estudantil\n",
    "\n",
    "##  Estrutura da An√°lise\n",
    "1. **Configura√ß√£o e Carregamento** - Setup inicial e importa√ß√£o dos dados\n",
    "2. **Explora√ß√£o Inicial** - Primeira vis√£o dos dados e estrutura\n",
    "3. **An√°lise de Qualidade** - Verifica√ß√£o de valores ausentes e inconsist√™ncias\n",
    "4. **Tratamento de Dados** - Limpeza e prepara√ß√£o dos dados\n",
    "5. **Vari√°veis Derivadas** - Cria√ß√£o de novas m√©tricas\n",
    "6. **Visualiza√ß√µes** - Gr√°ficos e an√°lises explorat√≥rias\n",
    "7. **Insights e Recomenda√ß√µes** - Conclus√µes e sugest√µes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456bcf9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 1. CONFIGURA√á√ÉO E CARREGAMENTO DE DADOS\n",
    "# =============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configura√ß√µes para visualiza√ß√µes de alta qualidade\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 11\n",
    "plt.rcParams['axes.titlesize'] = 14\n",
    "plt.rcParams['axes.labelsize'] = 12\n",
    "plt.rcParams['xtick.labelsize'] = 10\n",
    "plt.rcParams['ytick.labelsize'] = 10\n",
    "plt.rcParams['legend.fontsize'] = 10\n",
    "\n",
    "# Carregar os dados\n",
    "data_path = Path(\"../../data/habitos_e_desempenho_estudantil.csv\")\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "print(f\"Dados carregados: {data_path}\")\n",
    "print(f\"Dimens√µes do dataset: {df.shape[0]:,} linhas x {df.shape[1]} colunas\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679f7db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 2. EXPLORA√á√ÉO INICIAL\n",
    "# =============================================================================\n",
    "\n",
    "print(\"INFORMA√á√ïES B√ÅSICAS DO DATASET\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"‚Ä¢ Total de registros: {df.shape[0]:,}\")\n",
    "print(f\"‚Ä¢ Total de vari√°veis: {df.shape[1]}\")\n",
    "print(f\"‚Ä¢ Mem√≥ria utilizada: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "# Informa√ß√µes sobre tipos de dados\n",
    "print(\"\\n TIPOS DE DADOS\")\n",
    "print(\"=\" * 30)\n",
    "print(df.dtypes.value_counts())\n",
    "\n",
    "# Primeiras linhas\n",
    "print(\"\\n PRIMEIRAS 5 LINHAS\")\n",
    "print(\"=\" * 30)\n",
    "display(df.head())\n",
    "\n",
    "# Informa√ß√µes gerais\n",
    "print(\"\\n INFORMA√á√ïES GERAIS\")\n",
    "print(\"=\" * 30)\n",
    "df.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc63c18",
   "metadata": {},
   "source": [
    "## üîç Explora√ß√£o Inicial dos Dados\n",
    "\n",
    "Esta se√ß√£o fornece uma vis√£o geral dos dados, incluindo:\n",
    "- **Dimens√µes do dataset** - N√∫mero de registros e vari√°veis\n",
    "- **Tipos de dados** - Identifica√ß√£o de vari√°veis num√©ricas e categ√≥ricas\n",
    "- **Primeiras observa√ß√µes** - Amostra dos dados para entender a estrutura\n",
    "- **Informa√ß√µes gerais** - Resumo estat√≠stico b√°sico\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d845253b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 3. AN√ÅLISE DE QUALIDADE\n",
    "#\n",
    "# 3.1 VALORES AUSENTES\n",
    "# =============================================================================\n",
    "print(\"üìä AN√ÅLISE DE VALORES AUSENTES\")\n",
    "print(\"=\" * 40)\n",
    "missing_data = df.isnull().sum()\n",
    "missing_percent = (missing_data / len(df)) * 100\n",
    "missing_df = pd.DataFrame({\n",
    "    'Valores_Ausentes': missing_data,\n",
    "    'Percentual': missing_percent\n",
    "}).sort_values('Valores_Ausentes', ascending=False)\n",
    "\n",
    "if missing_df['Valores_Ausentes'].sum() > 0:\n",
    "    display(missing_df[missing_df['Valores_Ausentes'] > 0])\n",
    "else:\n",
    "    print(\"‚úÖ Nenhum valor ausente encontrado!\")\n",
    "\n",
    "# =============================================================================\n",
    "# 3.2 DETEC√á√ÉO DE VALORES NEGATIVOS E INCONSIST√äNCIAS\n",
    "# =============================================================================\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Verificar valores negativos em vari√°veis que n√£o deveriam ter\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "negative_values = {}\n",
    "\n",
    "for col in numeric_cols:\n",
    "    if col not in ['age']:  # age pode ser negativo em alguns contextos\n",
    "        neg_count = (df[col] < 0).sum()\n",
    "        if neg_count > 0:\n",
    "            negative_values[col] = neg_count\n",
    "            print(f\"‚ö†Ô∏è  {col}: {neg_count} valores negativos encontrados\")\n",
    "\n",
    "if not negative_values:\n",
    "    print(\"‚úÖ Nenhum valor negativo encontrado!\")\n",
    "\n",
    "# =============================================================================\n",
    "# 3.3 DETEC√á√ÉO DE OUTLIERS EXTREMOS\n",
    "# =============================================================================\n",
    "print(\"\\n DETEC√á√ÉO DE OUTLIERS EXTREMOS\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "outliers_summary = {}\n",
    "for col in numeric_cols:\n",
    "    Q1 = df[col].quantile(0.25)\n",
    "    Q3 = df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 3 * IQR  # Usando 3*IQR para detectar outliers extremos\n",
    "    upper_bound = Q3 + 3 * IQR\n",
    "    \n",
    "    outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]\n",
    "    outlier_count = len(outliers)\n",
    "    \n",
    "    if outlier_count > 0:\n",
    "        outliers_summary[col] = {\n",
    "            'count': outlier_count,\n",
    "            'percentage': (outlier_count / len(df)) * 100,\n",
    "            'min_outlier': outliers[col].min(),\n",
    "            'max_outlier': outliers[col].max()\n",
    "        }\n",
    "        print(f\"üîç {col}: {outlier_count} outliers extremos ({outlier_count/len(df)*100:.1f}%)\")\n",
    "        print(f\"   Range normal: [{lower_bound:.2f}, {upper_bound:.2f}]\")\n",
    "        print(f\"   Outliers: [{outliers[col].min():.2f}, {outliers[col].max():.2f}]\")\n",
    "\n",
    "if not outliers_summary:\n",
    "    print(\"‚úÖ Nenhum outlier extremo detectado!\")\n",
    "\n",
    "# =============================================================================\n",
    "# 3.4 AN√ÅLISE DE INCONSIST√äNCIAS L√ìGICAS\n",
    "# =============================================================================\n",
    "print(\"\\nüîç AN√ÅLISE DE INCONSIST√äNCIAS L√ìGICAS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Verificar inconsist√™ncias l√≥gicas\n",
    "inconsistencies = []\n",
    "\n",
    "# 1. Horas de estudo + sono + lazer > 24h\n",
    "df['total_daily_hours'] = df['study_hours_per_day'] + df['sleep_hours'] + df['social_media_hours'] + df['netflix_hours']\n",
    "excessive_hours = df[df['total_daily_hours'] > 24]\n",
    "if len(excessive_hours) > 0:\n",
    "    inconsistencies.append(f\"‚è∞ {len(excessive_hours)} registros com mais de 24h de atividades di√°rias\")\n",
    "\n",
    "# 2. Notas imposs√≠veis (fora do range 0-100)\n",
    "invalid_scores = df[(df['exam_score'] < 0) | (df['exam_score'] > 100)]\n",
    "if len(invalid_scores) > 0:\n",
    "    inconsistencies.append(f\"üìä {len(invalid_scores)} registros com notas inv√°lidas (fora de 0-100)\")\n",
    "\n",
    "# 3. Percentual de presen√ßa > 100%\n",
    "invalid_attendance = df[df['attendance_percentage'] > 100]\n",
    "if len(invalid_attendance) > 0:\n",
    "    inconsistencies.append(f\"üìÖ {len(invalid_attendance)} registros com presen√ßa > 100%\")\n",
    "\n",
    "# 4. Idades inconsistentes\n",
    "invalid_ages = df[(df['age'] < 16) | (df['age'] > 30)]\n",
    "if len(invalid_ages) > 0:\n",
    "    inconsistencies.append(f\"üë§ {len(invalid_ages)} registros com idades inconsistentes (16-30 anos)\")\n",
    "\n",
    "# 5. Sa√∫de mental fora do range 1-10\n",
    "invalid_mental_health = df[(df['mental_health_rating'] < 1) | (df['mental_health_rating'] > 10)]\n",
    "if len(invalid_mental_health) > 0:\n",
    "    inconsistencies.append(f\"üß† {len(invalid_mental_health)} registros com sa√∫de mental fora do range 1-10\")\n",
    "\n",
    "if inconsistencies:\n",
    "    print(\"üö® INCONSIST√äNCIAS DETECTADAS:\")\n",
    "    for inconsistency in inconsistencies:\n",
    "        print(f\"  {inconsistency}\")\n",
    "else:\n",
    "    print(\"‚úÖ Nenhuma inconsist√™ncia l√≥gica detectada!\")\n",
    "\n",
    "# =============================================================================\n",
    "# 3.5 DUPLICATAS E DADOS √öNICOS\n",
    "# =============================================================================\n",
    "print(\"\\nüîÑ AN√ÅLISE DE DUPLICATAS\")\n",
    "print(\"=\" * 30)\n",
    "duplicates = df.duplicated().sum()\n",
    "print(f\"Duplicatas completas: {duplicates}\")\n",
    "\n",
    "# Verificar duplicatas por ID\n",
    "duplicate_ids = df['student_id'].duplicated().sum()\n",
    "print(f\"IDs duplicados: {duplicate_ids}\")\n",
    "\n",
    "print(\"\\nüéØ VALORES √öNICOS POR COLUNA\")\n",
    "print(\"=\" * 40)\n",
    "unique_counts = df.nunique()\n",
    "for col in df.columns:\n",
    "    print(f\"{col}: {unique_counts[col]} valores √∫nicos\")\n",
    "\n",
    "# =============================================================================\n",
    "# 3.6 RESUMO DE QUALIDADE DOS DADOS\n",
    "# =============================================================================\n",
    "print(\"\\nüìã RESUMO DE QUALIDADE DOS DADOS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"‚Ä¢ Total de registros: {len(df):,}\")\n",
    "print(f\"‚Ä¢ Total de vari√°veis: {len(df.columns)}\")\n",
    "print(f\"‚Ä¢ Valores ausentes: {missing_df['Valores_Ausentes'].sum()}\")\n",
    "print(f\"‚Ä¢ Valores negativos inesperados: {sum(negative_values.values())}\")\n",
    "print(f\"‚Ä¢ Outliers extremos: {sum([info['count'] for info in outliers_summary.values()])}\")\n",
    "print(f\"‚Ä¢ Inconsist√™ncias l√≥gicas: {len(inconsistencies)}\")\n",
    "print(f\"‚Ä¢ Duplicatas: {duplicates}\")\n",
    "\n",
    "# Calcular score de qualidade\n",
    "quality_score = 100\n",
    "if missing_df['Valores_Ausentes'].sum() > 0:\n",
    "    quality_score -= 10\n",
    "if negative_values:\n",
    "    quality_score -= 15\n",
    "if outliers_summary:\n",
    "    quality_score -= 10\n",
    "if inconsistencies:\n",
    "    quality_score -= 20\n",
    "if duplicates > 0:\n",
    "    quality_score -= 5\n",
    "\n",
    "print(f\"\\n SCORE DE QUALIDADE DOS DADOS: {quality_score}/100\")\n",
    "\n",
    "print(\"\\nüìà ESTAT√çSTICAS DESCRITIVAS\")\n",
    "print(\"=\" * 40)\n",
    "display(df.describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc741c0",
   "metadata": {},
   "source": [
    "## üîß An√°lise de Qualidade dos Dados\n",
    "\n",
    "Esta se√ß√£o identifica e trata problemas de qualidade nos dados:\n",
    "\n",
    "### üìä Verifica√ß√µes Realizadas:\n",
    "- **Valores Ausentes** - Identifica√ß√£o de campos vazios\n",
    "- **Valores Negativos** - Detec√ß√£o de inconsist√™ncias l√≥gicas\n",
    "- **Outliers Extremos** - Identifica√ß√£o de valores at√≠picos\n",
    "- **Duplicatas** - Verifica√ß√£o de registros duplicados\n",
    "- **Inconsist√™ncias L√≥gicas** - Valida√ß√£o de regras de neg√≥cio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9711934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 4. TRATAMENTO DE DADOS INCONSISTENTES\n",
    "# =============================================================================\n",
    "\n",
    "print(\" TRATAMENTO DE DADOS INCONSISTENTES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Criar c√≥pia para tratamento\n",
    "df_clean = df.copy()\n",
    "\n",
    "# =============================================================================\n",
    "# 4.1 TRATAMENTO DE OUTLIERS EXTREMOS\n",
    "# =============================================================================\n",
    "print(\" TRATAMENTO DE OUTLIERS EXTREMOS\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Fun√ß√£o para tratar outliers usando winsorization\n",
    "def winsorize_outliers(series, lower_percentile=1, upper_percentile=99):\n",
    "    \"\"\"Aplica winsorization para tratar outliers extremos\"\"\"\n",
    "    lower_bound = series.quantile(lower_percentile / 100)\n",
    "    upper_bound = series.quantile(upper_percentile / 100)\n",
    "    \n",
    "    # Contar outliers antes do tratamento\n",
    "    outliers_before = ((series < lower_bound) | (series > upper_bound)).sum()\n",
    "    \n",
    "    # Aplicar winsorization\n",
    "    series_clean = series.clip(lower=lower_bound, upper=upper_bound)\n",
    "    \n",
    "    return series_clean, outliers_before\n",
    "\n",
    "# Tratar outliers em vari√°veis num√©ricas importantes\n",
    "numeric_vars_to_clean = ['study_hours_per_day', 'social_media_hours', 'netflix_hours', \n",
    "                        'attendance_percentage', 'sleep_hours', 'exam_score']\n",
    "\n",
    "outliers_treated = {}\n",
    "for var in numeric_vars_to_clean:\n",
    "    if var in df_clean.columns:\n",
    "        df_clean[var], outliers_count = winsorize_outliers(df_clean[var])\n",
    "        if outliers_count > 0:\n",
    "            outliers_treated[var] = outliers_count\n",
    "            print(f\"üîß {var}: {outliers_count} outliers tratados\")\n",
    "\n",
    "if outliers_treated:\n",
    "    print(f\"\\n‚úÖ Total de outliers tratados: {sum(outliers_treated.values())}\")\n",
    "else:\n",
    "    print(\"‚úÖ Nenhum outlier extremo encontrado para tratamento!\")\n",
    "\n",
    "# =============================================================================\n",
    "# 4.2 TRATAMENTO DE INCONSIST√äNCIAS L√ìGICAS\n",
    "# =============================================================================\n",
    "print(\"\\nüîç TRATAMENTO DE INCONSIST√äNCIAS L√ìGICAS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. Corrigir horas totais excessivas (cap em 24h)\n",
    "df_clean['total_daily_hours'] = (df_clean['study_hours_per_day'] + \n",
    "                                df_clean['sleep_hours'] + \n",
    "                                df_clean['social_media_hours'] + \n",
    "                                df_clean['netflix_hours'])\n",
    "\n",
    "excessive_hours = df_clean[df_clean['total_daily_hours'] > 24]\n",
    "if len(excessive_hours) > 0:\n",
    "    print(f\"‚è∞ Ajustando {len(excessive_hours)} registros com horas excessivas\")\n",
    "    # Reduzir proporcionalmente as atividades\n",
    "    for idx in excessive_hours.index:\n",
    "        total = df_clean.loc[idx, 'total_daily_hours']\n",
    "        scale_factor = 24 / total\n",
    "        df_clean.loc[idx, 'study_hours_per_day'] *= scale_factor\n",
    "        df_clean.loc[idx, 'social_media_hours'] *= scale_factor\n",
    "        df_clean.loc[idx, 'netflix_hours'] *= scale_factor\n",
    "\n",
    "# 2. Corrigir notas fora do range 0-100\n",
    "invalid_scores = df_clean[(df_clean['exam_score'] < 0) | (df_clean['exam_score'] > 100)]\n",
    "if len(invalid_scores) > 0:\n",
    "    print(f\"üìä Corrigindo {len(invalid_scores)} notas inv√°lidas\")\n",
    "    df_clean['exam_score'] = df_clean['exam_score'].clip(0, 100)\n",
    "\n",
    "# 3. Corrigir presen√ßa > 100%\n",
    "invalid_attendance = df_clean[df_clean['attendance_percentage'] > 100]\n",
    "if len(invalid_attendance) > 0:\n",
    "    print(f\"üìÖ Corrigindo {len(invalid_attendance)} presen√ßas inv√°lidas\")\n",
    "    df_clean['attendance_percentage'] = df_clean['attendance_percentage'].clip(0, 100)\n",
    "\n",
    "# 4. Corrigir idades inconsistentes\n",
    "invalid_ages = df_clean[(df_clean['age'] < 16) | (df_clean['age'] > 30)]\n",
    "if len(invalid_ages) > 0:\n",
    "    print(f\"üë§ Corrigindo {len(invalid_ages)} idades inconsistentes\")\n",
    "    df_clean['age'] = df_clean['age'].clip(16, 30)\n",
    "\n",
    "# 5. Corrigir sa√∫de mental fora do range 1-10\n",
    "invalid_mental_health = df_clean[(df_clean['mental_health_rating'] < 1) | (df_clean['mental_health_rating'] > 10)]\n",
    "if len(invalid_mental_health) > 0:\n",
    "    print(f\"üß† Corrigindo {len(invalid_mental_health)} avalia√ß√µes de sa√∫de mental inv√°lidas\")\n",
    "    df_clean['mental_health_rating'] = df_clean['mental_health_rating'].clip(1, 10)\n",
    "\n",
    "# =============================================================================\n",
    "# 4.3 TRATAMENTO DE VALORES AUSENTES\n",
    "# =============================================================================\n",
    "print(\"\\nüìä TRATAMENTO DE VALORES AUSENTES\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Verificar valores ausentes ap√≥s limpeza\n",
    "missing_after_clean = df_clean.isnull().sum()\n",
    "if missing_after_clean.sum() > 0:\n",
    "    print(\"Valores ausentes encontrados:\")\n",
    "    for col, count in missing_after_clean[missing_after_clean > 0].items():\n",
    "        print(f\"  {col}: {count} valores ausentes\")\n",
    "        \n",
    "        # Estrat√©gias de imputa√ß√£o baseadas no tipo de vari√°vel\n",
    "        if col in ['parental_education_level']:\n",
    "            # Para vari√°veis categ√≥ricas, usar moda\n",
    "            mode_value = df_clean[col].mode()[0]\n",
    "            df_clean[col].fillna(mode_value, inplace=True)\n",
    "            print(f\"    ‚Üí Preenchido com moda: {mode_value}\")\n",
    "        elif col in ['age', 'exercise_frequency', 'mental_health_rating']:\n",
    "            # Para vari√°veis num√©ricas discretas, usar mediana\n",
    "            median_value = df_clean[col].median()\n",
    "            df_clean[col].fillna(median_value, inplace=True)\n",
    "            print(f\"    ‚Üí Preenchido com mediana: {median_value}\")\n",
    "        else:\n",
    "            # Para outras vari√°veis num√©ricas, usar m√©dia\n",
    "            mean_value = df_clean[col].mean()\n",
    "            df_clean[col].fillna(mean_value, inplace=True)\n",
    "            print(f\"    ‚Üí Preenchido com m√©dia: {mean_value:.2f}\")\n",
    "else:\n",
    "    print(\"‚úÖ Nenhum valor ausente encontrado!\")\n",
    "\n",
    "# =============================================================================\n",
    "# 4.4 VERIFICA√á√ÉO FINAL DE QUALIDADE\n",
    "# =============================================================================\n",
    "print(\"\\n‚úÖ VERIFICA√á√ÉO FINAL DE QUALIDADE\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Recalcular m√©tricas de qualidade\n",
    "final_missing = df_clean.isnull().sum().sum()\n",
    "final_duplicates = df_clean.duplicated().sum()\n",
    "final_negative = (df_clean.select_dtypes(include=[np.number]) < 0).sum().sum()\n",
    "\n",
    "print(f\"‚Ä¢ Valores ausentes: {final_missing}\")\n",
    "print(f\"‚Ä¢ Duplicatas: {final_duplicates}\")\n",
    "print(f\"‚Ä¢ Valores negativos: {final_negative}\")\n",
    "\n",
    "# Verificar se ainda h√° inconsist√™ncias\n",
    "final_inconsistencies = []\n",
    "final_inconsistencies.extend(df_clean[df_clean['exam_score'] < 0].index.tolist())\n",
    "final_inconsistencies.extend(df_clean[df_clean['exam_score'] > 100].index.tolist())\n",
    "final_inconsistencies.extend(df_clean[df_clean['attendance_percentage'] > 100].index.tolist())\n",
    "\n",
    "print(f\"‚Ä¢ Inconsist√™ncias restantes: {len(set(final_inconsistencies))}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb652bc",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Tratamento de Dados Inconsistentes\n",
    "\n",
    "Esta se√ß√£o aplica corre√ß√µes e limpezas nos dados identificados:\n",
    "\n",
    "### üîÑ Processos de Limpeza:\n",
    "- **Tratamento de Valores Ausentes** - Preenchimento ou remo√ß√£o de dados faltantes\n",
    "- **Corre√ß√£o de Inconsist√™ncias** - Ajuste de valores que n√£o fazem sentido\n",
    "- **Padroniza√ß√£o** - Uniformiza√ß√£o de formatos e categorias\n",
    "- **Valida√ß√£o Final** - Verifica√ß√£o da qualidade ap√≥s tratamento\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3953118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 5. COMPARATIVO VISUAL: ANTES vs DEPOIS DO TRATAMENTO\n",
    "# =============================================================================\n",
    "\n",
    "print(\"üìä COMPARATIVO VISUAL: ANTES vs DEPOIS DO TRATAMENTO\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Configurar subplots para compara√ß√£o\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('Comparativo: Dados Originais vs Dados Tratados', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Vari√°veis principais para compara√ß√£o\n",
    "main_vars = ['study_hours_per_day', 'social_media_hours', 'netflix_hours', \n",
    "            'attendance_percentage', 'sleep_hours', 'exam_score']\n",
    "\n",
    "# Cores para distinguir antes/depois\n",
    "colors_before = ['#ff6b6b', '#4ecdc4', '#45b7d1', '#96ceb4', '#feca57', '#ff9ff3']\n",
    "colors_after = ['#ff8e8e', '#6ed5c7', '#6bc5d8', '#a8d5c4', '#fed766', '#ffb3f3']\n",
    "\n",
    "for i, var in enumerate(main_vars):\n",
    "    row = i // 3\n",
    "    col = i % 3\n",
    "    ax = axes[row, col]\n",
    "    \n",
    "    # Dados originais\n",
    "    original_data = df[var].dropna()\n",
    "    treated_data = df_clean[var].dropna()\n",
    "    \n",
    "    # Criar histogramas sobrepostos\n",
    "    ax.hist(original_data, bins=30, alpha=0.7, label='Antes', \n",
    "            color=colors_before[i], density=True)\n",
    "    ax.hist(treated_data, bins=30, alpha=0.7, label='Depois', \n",
    "            color=colors_after[i], density=True)\n",
    "    \n",
    "    # Configura√ß√µes do gr√°fico\n",
    "    ax.set_title(f'{var.replace(\"_\", \" \").title()}', fontweight='bold')\n",
    "    ax.set_xlabel('Valor')\n",
    "    ax.set_ylabel('Densidade')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Adicionar estat√≠sticas\n",
    "    orig_mean = original_data.mean()\n",
    "    treat_mean = treated_data.mean()\n",
    "    orig_std = original_data.std()\n",
    "    treat_std = treated_data.std()\n",
    "    \n",
    "    ax.text(0.02, 0.98, f'Antes: Œº={orig_mean:.2f}, œÉ={orig_std:.2f}', \n",
    "            transform=ax.transAxes, verticalalignment='top',\n",
    "            bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    ax.text(0.02, 0.88, f'Depois: Œº={treat_mean:.2f}, œÉ={treat_std:.2f}', \n",
    "            transform=ax.transAxes, verticalalignment='top',\n",
    "            bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# =============================================================================\n",
    "# 5.1 AN√ÅLISE ESTAT√çSTICA DO TRATAMENTO\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\nüìà AN√ÅLISE ESTAT√çSTICA DO TRATAMENTO\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Criar DataFrame comparativo\n",
    "comparison_stats = []\n",
    "\n",
    "for var in main_vars:\n",
    "    if var in df.columns and var in df_clean.columns:\n",
    "        orig_data = df[var].dropna()\n",
    "        treat_data = df_clean[var].dropna()\n",
    "        \n",
    "        stats = {\n",
    "            'Vari√°vel': var.replace('_', ' ').title(),\n",
    "            'Antes_Media': orig_data.mean(),\n",
    "            'Depois_Media': treat_data.mean(),\n",
    "            'Antes_Desvio': orig_data.std(),\n",
    "            'Depois_Desvio': treat_data.std(),\n",
    "            'Antes_Min': orig_data.min(),\n",
    "            'Depois_Min': treat_data.min(),\n",
    "            'Antes_Max': orig_data.max(),\n",
    "            'Depois_Max': treat_data.max(),\n",
    "            'Diferenca_Media': treat_data.mean() - orig_data.mean(),\n",
    "            'Diferenca_Desvio': treat_data.std() - orig_data.std()\n",
    "        }\n",
    "        comparison_stats.append(stats)\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_stats)\n",
    "\n",
    "# Exibir tabela comparativa\n",
    "print(\"üìä ESTAT√çSTICAS COMPARATIVAS\")\n",
    "print(\"=\" * 40)\n",
    "display(comparison_df.round(3))\n",
    "\n",
    "# =============================================================================\n",
    "# 5.2 AN√ÅLISE DE IMPACTO DO TRATAMENTO\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\nüéØ AN√ÅLISE DE IMPACTO DO TRATAMENTO\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Calcular m√©tricas de impacto\n",
    "impact_analysis = []\n",
    "\n",
    "for var in main_vars:\n",
    "    if var in df.columns and var in df_clean.columns:\n",
    "        orig_data = df[var].dropna()\n",
    "        treat_data = df_clean[var].dropna()\n",
    "        \n",
    "        # M√©tricas de impacto\n",
    "        mean_change = ((treat_data.mean() - orig_data.mean()) / orig_data.mean()) * 100\n",
    "        std_change = ((treat_data.std() - orig_data.std()) / orig_data.std()) * 100\n",
    "        range_change = ((treat_data.max() - treat_data.min()) - (orig_data.max() - orig_data.min())) / (orig_data.max() - orig_data.min()) * 100\n",
    "        \n",
    "        impact_analysis.append({\n",
    "            'Vari√°vel': var.replace('_', ' ').title(),\n",
    "            'Mudan√ßa_M√©dia_%': mean_change,\n",
    "            'Mudan√ßa_Desvio_%': std_change,\n",
    "            'Mudan√ßa_Range_%': range_change,\n",
    "            'Impacto': 'Alto' if abs(mean_change) > 5 else 'Baixo' if abs(mean_change) < 1 else 'Moderado'\n",
    "        })\n",
    "\n",
    "impact_df = pd.DataFrame(impact_analysis)\n",
    "display(impact_df.round(2))\n",
    "\n",
    "# =============================================================================\n",
    "# 5.3 VISUALIZA√á√ÉO DE BOXPLOTS COMPARATIVOS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\nüì¶ BOXPLOTS COMPARATIVOS\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Criar boxplots comparativos\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "fig.suptitle('Boxplots Comparativos: Antes vs Depois do Tratamento', fontsize=16, fontweight='bold')\n",
    "\n",
    "for i, var in enumerate(main_vars):\n",
    "    row = i // 3\n",
    "    col = i % 3\n",
    "    ax = axes[row, col]\n",
    "    \n",
    "    # Preparar dados para boxplot\n",
    "    data_to_plot = [df[var].dropna(), df_clean[var].dropna()]\n",
    "    labels = ['Antes', 'Depois']\n",
    "    \n",
    "    # Criar boxplot\n",
    "    bp = ax.boxplot(data_to_plot, labels=labels, patch_artist=True)\n",
    "    \n",
    "    # Colorir as caixas\n",
    "    bp['boxes'][0].set_facecolor('#ff6b6b')\n",
    "    bp['boxes'][1].set_facecolor('#4ecdc4')\n",
    "    \n",
    "    # Configura√ß√µes\n",
    "    ax.set_title(f'{var.replace(\"_\", \" \").title()}', fontweight='bold')\n",
    "    ax.set_ylabel('Valor')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Adicionar estat√≠sticas no gr√°fico\n",
    "    orig_q1, orig_q3 = df[var].quantile([0.25, 0.75])\n",
    "    treat_q1, treat_q3 = df_clean[var].quantile([0.25, 0.75])\n",
    "    \n",
    "    ax.text(0.02, 0.98, f'Q1-Q3 Antes: {orig_q1:.1f}-{orig_q3:.1f}', \n",
    "            transform=ax.transAxes, verticalalignment='top',\n",
    "            bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    ax.text(0.02, 0.88, f'Q1-Q3 Depois: {treat_q1:.1f}-{treat_q3:.1f}', \n",
    "            transform=ax.transAxes, verticalalignment='top',\n",
    "            bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# =============================================================================\n",
    "# 5.4 RESUMO DO TRATAMENTO\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\nüìã RESUMO DO TRATAMENTO REALIZADO\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Contar mudan√ßas significativas\n",
    "significant_changes = impact_df[impact_df['Mudan√ßa_M√©dia_%'].abs() > 1]\n",
    "moderate_changes = impact_df[(impact_df['Mudan√ßa_M√©dia_%'].abs() > 0.5) & (impact_df['Mudan√ßa_M√©dia_%'].abs() <= 1)]\n",
    "minimal_changes = impact_df[impact_df['Mudan√ßa_M√©dia_%'].abs() <= 0.5]\n",
    "\n",
    "print(f\"üîç Mudan√ßas Significativas (>1%): {len(significant_changes)} vari√°veis\")\n",
    "print(f\"üîç Mudan√ßas Moderadas (0.5-1%): {len(moderate_changes)} vari√°veis\")\n",
    "print(f\"üîç Mudan√ßas M√≠nimas (<0.5%): {len(minimal_changes)} vari√°veis\")\n",
    "\n",
    "print(f\"\\n‚úÖ Tratamento conclu√≠do com sucesso!\")\n",
    "print(f\"üìä Total de vari√°veis analisadas: {len(main_vars)}\")\n",
    "print(f\"üìà Dados originais preservados: {len(df):,} registros\")\n",
    "print(f\"üìà Dados tratados: {len(df_clean):,} registros\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0646ae70",
   "metadata": {},
   "source": [
    "## üìà Cria√ß√£o de Vari√°veis Derivadas\n",
    "\n",
    "Esta se√ß√£o cria novas m√©tricas e indicadores para enriquecer a an√°lise:\n",
    "\n",
    "### üÜï Novas Vari√°veis Criadas:\n",
    "- **√çndices Compostos** - M√©tricas que combinam m√∫ltiplas vari√°veis\n",
    "- **Categoriza√ß√µes** - Segmenta√ß√£o de vari√°veis cont√≠nuas\n",
    "- **Ratios e Propor√ß√µes** - Rela√ß√µes entre diferentes m√©tricas\n",
    "- **Indicadores de Risco** - Flags para identificar estudantes em risco\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e0bc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 5. VARI√ÅVEIS DERIVADAS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"VARI√ÅVEIS DERIVADAS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Usar o dataset limpo como base\n",
    "df_processed = df_clean.copy()\n",
    "\n",
    "# =============================================================================\n",
    "# 5.1 VARI√ÅVEIS CATEG√ìRICAS\n",
    "# =============================================================================\n",
    "\n",
    "# Fun√ß√µes de categoriza√ß√£o\n",
    "def categorize_social_media_usage(hours):\n",
    "    if hours <= 1: return 'Baixo'\n",
    "    elif hours <= 3: return 'Moderado'\n",
    "    elif hours <= 5: return 'Alto'\n",
    "    else: return 'Muito Alto'\n",
    "\n",
    "def categorize_study_hours(hours):\n",
    "    if hours <= 2: return 'Baixo'\n",
    "    elif hours <= 4: return 'Moderado'\n",
    "    elif hours <= 6: return 'Alto'\n",
    "    else: return 'Muito Alto'\n",
    "\n",
    "def categorize_sleep_hours(hours):\n",
    "    if hours < 6: return 'Insuficiente'\n",
    "    elif hours <= 7: return 'Adequado'\n",
    "    elif hours <= 9: return 'Ideal'\n",
    "    else: return 'Excessivo'\n",
    "\n",
    "def categorize_academic_performance(score):\n",
    "    if score < 50: return 'Baixo'\n",
    "    elif score < 70: return 'Regular'\n",
    "    elif score < 85: return 'Bom'\n",
    "    else: return 'Excelente'\n",
    "\n",
    "# Aplicar categoriza√ß√µes\n",
    "df_processed['social_media_category'] = df_processed['social_media_hours'].apply(categorize_social_media_usage)\n",
    "df_processed['study_hours_category'] = df_processed['study_hours_per_day'].apply(categorize_study_hours)\n",
    "df_processed['sleep_category'] = df_processed['sleep_hours'].apply(categorize_sleep_hours)\n",
    "df_processed['performance_category'] = df_processed['exam_score'].apply(categorize_academic_performance)\n",
    "\n",
    "# =============================================================================\n",
    "# 5.2 VARI√ÅVEIS NUM√âRICAS DERIVADAS\n",
    "# =============================================================================\n",
    "\n",
    "# √çndice de bem-estar\n",
    "df_processed['wellness_index'] = (\n",
    "    df_processed['sleep_hours'] * 0.4 + \n",
    "    df_processed['exercise_frequency'] * 0.3 + \n",
    "    df_processed['mental_health_rating'] * 0.3\n",
    ")\n",
    "\n",
    "# Horas totais de lazer\n",
    "df_processed['total_leisure_hours'] = (\n",
    "    df_processed['social_media_hours'] + \n",
    "    df_processed['netflix_hours']\n",
    ")\n",
    "\n",
    "# Tempo dispon√≠vel para estudo\n",
    "df_processed['available_study_time'] = (\n",
    "    24 - \n",
    "    df_processed['sleep_hours'] - \n",
    "    df_processed['total_leisure_hours'] - \n",
    "    df_processed['part_time_job'].map({'Yes': 8, 'No': 0})\n",
    ")\n",
    "\n",
    "# Raz√£o estudo/entretenimento\n",
    "df_processed['study_screen_ratio'] = (\n",
    "    df_processed['study_hours_per_day'] / \n",
    "    (df_processed['total_leisure_hours'] + 0.1)  # +0.1 para evitar divis√£o por zero\n",
    ")\n",
    "\n",
    "# Flag de underperformers\n",
    "df_processed['underperformer_flag'] = (\n",
    "    ((df_processed['attendance_percentage'] > 80) & (df_processed['exam_score'] < 60)) |\n",
    "    ((df_processed['study_hours_per_day'] > 4) & (df_processed['exam_score'] < 50))\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Vari√°veis derivadas criadas com sucesso!\")\n",
    "print(\"üìä Total de vari√°veis no dataset: {df_processed.shape[1]}\")\n",
    "print(\"üìà Novas vari√°veis adicionadas: 9\")\n",
    "\n",
    "# =============================================================================\n",
    "# 5.3 AN√ÅLISE DAS NOVAS VARI√ÅVEIS\n",
    "# =============================================================================\n",
    "\n",
    "# Estat√≠sticas das novas vari√°veis num√©ricas\n",
    "print(\"\\nüìà ESTAT√çSTICAS DAS NOVAS VARI√ÅVEIS NUM√âRICAS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "new_numeric_vars = ['total_leisure_hours', 'available_study_time', 'study_screen_ratio', 'wellness_index']\n",
    "for var in new_numeric_vars:\n",
    "    print(f\"\\n{var}:\")\n",
    "    print(f\"  ‚Ä¢ M√©dia: {df_processed[var].mean():.2f}\")\n",
    "    print(f\"  ‚Ä¢ Mediana: {df_processed[var].median():.2f}\")\n",
    "    print(f\"  ‚Ä¢ Desvio Padr√£o: {df_processed[var].std():.2f}\")\n",
    "\n",
    "# An√°lise dos underperformers\n",
    "underperformers_count = df_processed['underperformer_flag'].sum()\n",
    "print(f\"\\nüéØ UNDERPERFORMERS IDENTIFICADOS: {underperformers_count} ({underperformers_count/len(df_processed)*100:.1f}%)\")\n",
    "\n",
    "# Correla√ß√µes das novas vari√°veis\n",
    "print(\"\\nüîç CORRELA√á√ïES COM DESEMPENHO ACAD√äMICO\")\n",
    "print(\"=\" * 60)\n",
    "for var in new_numeric_vars:\n",
    "    corr = df_processed[var].corr(df_processed['exam_score'])\n",
    "    direction = \"üìà\" if corr > 0 else \"üìâ\"\n",
    "    strength = \"Forte\" if abs(corr) > 0.5 else \"Moderada\" if abs(corr) > 0.3 else \"Fraca\"\n",
    "    print(f\"{direction} {var}: {corr:.3f} ({strength})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ab244d",
   "metadata": {},
   "source": [
    "## üìä Visualiza√ß√µes e An√°lises Explorat√≥rias\n",
    "\n",
    "Esta se√ß√£o apresenta gr√°ficos e an√°lises visuais dos dados:\n",
    "\n",
    "### üé® Tipos de Visualiza√ß√µes:\n",
    "- **Distribui√ß√µes** - Histogramas e gr√°ficos de densidade\n",
    "- **Correla√ß√µes** - Mapas de calor e matrizes de correla√ß√£o\n",
    "- **Compara√ß√µes** - Gr√°ficos de barras e boxplots\n",
    "- **Tend√™ncias** - An√°lises temporais e padr√µes comportamentais\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa21f897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 6. VISUALIZA√á√ïES\n",
    "# =============================================================================\n",
    "\n",
    "print(\"üìä VISUALIZA√á√ïES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Configura√ß√µes avan√ßadas para visualiza√ß√µes\n",
    "plt.style.use('default')\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams.update({\n",
    "    'figure.figsize': (16, 10),\n",
    "    'font.size': 12,\n",
    "    'axes.titlesize': 16,\n",
    "    'axes.labelsize': 14,\n",
    "    'xtick.labelsize': 12,\n",
    "    'ytick.labelsize': 12,\n",
    "    'legend.fontsize': 12,\n",
    "    'axes.spines.top': False,\n",
    "    'axes.spines.right': False,\n",
    "    'axes.grid': True,\n",
    "    'grid.alpha': 0.3\n",
    "})\n",
    "\n",
    "# Paleta de cores profissional\n",
    "colors = ['#2E86AB', '#A23B72', '#F18F01', '#C73E1D', '#7209B7', '#F77F00', '#06FFA5']\n",
    "sns.set_palette(colors)\n",
    "\n",
    "# =============================================================================\n",
    "# 6.1 VISUALIZA√á√ÉO PRINCIPAL\n",
    "# =============================================================================\n",
    "\n",
    "fig = plt.figure(figsize=(20, 16))\n",
    "gs = fig.add_gridspec(3, 4, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# 1. Distribui√ß√£o de Desempenho (Gr√°fico Principal)\n",
    "ax1 = fig.add_subplot(gs[0, :2])\n",
    "performance_counts = df_processed['performance_category'].value_counts()\n",
    "wedges, texts, autotexts = ax1.pie(performance_counts.values, \n",
    "                                  labels=performance_counts.index,\n",
    "                                  autopct='%1.1f%%',\n",
    "                                  startangle=90,\n",
    "                                  colors=colors[:len(performance_counts)],\n",
    "                                  explode=[0.05 if x == performance_counts.max() else 0 for x in performance_counts.values])\n",
    "ax1.set_title('Distribui√ß√£o de Desempenho Acad√™mico', fontsize=18, fontweight='bold', pad=20)\n",
    "ax1.axis('equal')\n",
    "\n",
    "# 2. Correla√ß√£o com Desempenho\n",
    "ax2 = fig.add_subplot(gs[0, 2:])\n",
    "numeric_cols = df_processed.select_dtypes(include=[np.number]).columns\n",
    "corr_matrix = df_processed[numeric_cols].corr()\n",
    "exam_corr = corr_matrix['exam_score'].drop('exam_score').sort_values(ascending=True)\n",
    "\n",
    "bars = ax2.barh(range(len(exam_corr)), exam_corr.values, \n",
    "                color=[colors[0] if x > 0 else colors[3] for x in exam_corr.values])\n",
    "ax2.set_yticks(range(len(exam_corr)))\n",
    "ax2.set_yticklabels([col.replace('_', ' ').title() for col in exam_corr.index])\n",
    "ax2.set_xlabel('Correla√ß√£o com Desempenho', fontsize=12)\n",
    "ax2.set_title('Fatores que Influenciam o Desempenho', fontsize=16, fontweight='bold')\n",
    "ax2.axvline(x=0, color='black', linestyle='-', alpha=0.3)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Adicionar valores nas barras\n",
    "for i, (bar, value) in enumerate(zip(bars, exam_corr.values)):\n",
    "    ax2.text(value + (0.01 if value > 0 else -0.01), bar.get_y() + bar.get_height()/2, \n",
    "             f'{value:.3f}', ha='left' if value > 0 else 'right', va='center', fontweight='bold')\n",
    "\n",
    "# 3. An√°lise de Horas de Estudo vs Desempenho (Scatter com Regress√£o)\n",
    "ax3 = fig.add_subplot(gs[1, :2])\n",
    "scatter = ax3.scatter(df_processed['study_hours_per_day'], df_processed['exam_score'], \n",
    "                     c=df_processed['mental_health_rating'], \n",
    "                     cmap='viridis', alpha=0.7, s=60, edgecolors='white', linewidth=0.5)\n",
    "\n",
    "# Linha de regress√£o\n",
    "z = np.polyfit(df_processed['study_hours_per_day'], df_processed['exam_score'], 1)\n",
    "p = np.poly1d(z)\n",
    "ax3.plot(df_processed['study_hours_per_day'], p(df_processed['study_hours_per_day']), \n",
    "         \"r--\", alpha=0.8, linewidth=3, label=f'Tend√™ncia (R¬≤={np.corrcoef(df_processed[\"study_hours_per_day\"], df_processed[\"exam_score\"])[0,1]**2:.3f})')\n",
    "\n",
    "ax3.set_xlabel('Horas de Estudo por Dia', fontsize=12)\n",
    "ax3.set_ylabel('Nota do Exame', fontsize=12)\n",
    "ax3.set_title('Rela√ß√£o entre Horas de Estudo e Desempenho\\n(Cor: Sa√∫de Mental)', fontsize=14, fontweight='bold')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Colorbar para sa√∫de mental\n",
    "cbar = plt.colorbar(scatter, ax=ax3, shrink=0.8)\n",
    "cbar.set_label('Sa√∫de Mental (1-10)', fontsize=10)\n",
    "\n",
    "# 4. Boxplot Comparativo por Categorias\n",
    "ax4 = fig.add_subplot(gs[1, 2:])\n",
    "categories = ['social_media_category', 'sleep_category', 'diet_quality']\n",
    "category_data = []\n",
    "category_labels = []\n",
    "\n",
    "for cat in categories:\n",
    "    if cat in df_processed.columns:\n",
    "        for value in df_processed[cat].unique():\n",
    "            subset = df_processed[df_processed[cat] == value]\n",
    "            category_data.append(subset['exam_score'].values)\n",
    "            category_labels.append(f\"{cat.replace('_', ' ').title()}\\n{value}\")\n",
    "\n",
    "box_plot = ax4.boxplot(category_data, labels=category_labels, patch_artist=True)\n",
    "for patch, color in zip(box_plot['boxes'], colors):\n",
    "    patch.set_facecolor(color)\n",
    "    patch.set_alpha(0.7)\n",
    "\n",
    "ax4.set_ylabel('Nota do Exame', fontsize=12)\n",
    "ax4.set_title('Desempenho por Categorias de H√°bitos', fontsize=14, fontweight='bold')\n",
    "ax4.tick_params(axis='x', rotation=45)\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "# 5. An√°lise Temporal/Sequencial (se houver dados temporais)\n",
    "ax5 = fig.add_subplot(gs[2, :2])\n",
    "# Criar um histograma 2D mostrando a distribui√ß√£o de horas de estudo vs redes sociais\n",
    "ax5.hist2d(df_processed['study_hours_per_day'], df_processed['social_media_hours'], \n",
    "          bins=20, cmap='Blues', alpha=0.8)\n",
    "ax5.set_xlabel('Horas de Estudo por Dia', fontsize=12)\n",
    "ax5.set_ylabel('Horas de Redes Sociais', fontsize=12)\n",
    "ax5.set_title('Distribui√ß√£o: Estudo vs Redes Sociais', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Adicionar contornos\n",
    "from scipy.stats import gaussian_kde\n",
    "xy = np.vstack([df_processed['study_hours_per_day'], df_processed['social_media_hours']])\n",
    "density = gaussian_kde(xy)\n",
    "x_range = np.linspace(df_processed['study_hours_per_day'].min(), df_processed['study_hours_per_day'].max(), 50)\n",
    "y_range = np.linspace(df_processed['social_media_hours'].min(), df_processed['social_media_hours'].max(), 50)\n",
    "X, Y = np.meshgrid(x_range, y_range)\n",
    "positions = np.vstack([X.ravel(), Y.ravel()])\n",
    "Z = np.reshape(density(positions).T, X.shape)\n",
    "ax5.contour(X, Y, Z, levels=5, colors='red', alpha=0.6, linewidths=2)\n",
    "\n",
    "# 6. M√©tricas de Performance (Gauge-style)\n",
    "ax6 = fig.add_subplot(gs[2, 2:])\n",
    "ax6.axis('off')\n",
    "\n",
    "# Calcular m√©tricas principais\n",
    "high_performers = len(df_processed[df_processed['performance_category'] == 'Excelente'])\n",
    "avg_score = df_processed['exam_score'].mean()\n",
    "study_correlation = df_processed['study_hours_per_day'].corr(df_processed['exam_score'])\n",
    "\n",
    "# Criar um \"dashboard\" com m√©tricas\n",
    "metrics_text = f\"\"\"\n",
    "üìä M√âTRICAS PRINCIPAIS\n",
    "\n",
    "üéØ Estudantes de Alto Desempenho\n",
    "{high_performers} ({high_performers/len(df_processed)*100:.1f}%)\n",
    "\n",
    "üìà Nota M√©dia Geral\n",
    "{avg_score:.1f}/100\n",
    "\n",
    "üîó Correla√ß√£o Estudo-Desempenho\n",
    "{study_correlation:.3f}\n",
    "\n",
    "‚ö° Fator Mais Importante\n",
    "Horas de Estudo\n",
    "\"\"\"\n",
    "\n",
    "ax6.text(0.1, 0.9, metrics_text, transform=ax6.transAxes, fontsize=14,\n",
    "         verticalalignment='top', bbox=dict(boxstyle=\"round,pad=0.5\", facecolor='lightblue', alpha=0.8))\n",
    "\n",
    "plt.suptitle(' AN√ÅLISE DE H√ÅBITOS X DESEMPENHO', \n",
    "             fontsize=20, fontweight='bold', y=0.98)\n",
    "plt.show()\n",
    "\n",
    "# =============================================================================\n",
    "# 6.2 VISUALIZA√á√ÉO DE CORRELA√á√ïES AVAN√áADA\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\nüìà MAPA DE CALOR DE CORRELA√á√ïES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "# Mapa de calor completo com m√°scara\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "sns.heatmap(corr_matrix, \n",
    "            mask=mask,\n",
    "            annot=True, \n",
    "            cmap='RdBu_r', \n",
    "            center=0,\n",
    "            square=True,\n",
    "            fmt='.2f',\n",
    "            cbar_kws={\"shrink\": .8},\n",
    "            ax=ax1,\n",
    "            linewidths=0.5)\n",
    "ax1.set_title('Matriz de Correla√ß√µes Completa', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Mapa de calor focado no desempenho\n",
    "exam_corr_data = corr_matrix[['exam_score']].sort_values('exam_score', ascending=False)\n",
    "sns.heatmap(exam_corr_data, \n",
    "            annot=True, \n",
    "            cmap='RdYlGn', \n",
    "            center=0,\n",
    "            fmt='.3f',\n",
    "            cbar_kws={\"shrink\": .8},\n",
    "            ax=ax2,\n",
    "            linewidths=0.5)\n",
    "ax2.set_title('Correla√ß√µes com Desempenho Acad√™mico', fontsize=16, fontweight='bold')\n",
    "ax2.set_xlabel('')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# =============================================================================\n",
    "# 6.3 AN√ÅLISE DE PERFIS DE ESTUDANTES\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\nüë• AN√ÅLISE DE PERFIS DE ESTUDANTES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Distribui√ß√£o por G√™nero\n",
    "gender_performance = df_processed.groupby(['gender', 'performance_category']).size().unstack(fill_value=0)\n",
    "gender_performance_pct = gender_performance.div(gender_performance.sum(axis=1), axis=0) * 100\n",
    "\n",
    "gender_performance_pct.plot(kind='bar', ax=axes[0,0], color=colors[:4])\n",
    "axes[0,0].set_title('Distribui√ß√£o de Desempenho por G√™nero', fontweight='bold')\n",
    "axes[0,0].set_xlabel('G√™nero')\n",
    "axes[0,0].set_ylabel('Percentual (%)')\n",
    "axes[0,0].legend(title='Desempenho', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "axes[0,0].tick_params(axis='x', rotation=0)\n",
    "\n",
    "# 2. An√°lise de Idade vs Desempenho\n",
    "age_bins = pd.cut(df_processed['age'], bins=5, labels=['16-18', '19-21', '22-24', '25-27', '28-30'])\n",
    "df_processed['age_group'] = age_bins\n",
    "age_performance = df_processed.groupby(['age_group', 'performance_category']).size().unstack(fill_value=0)\n",
    "age_performance_pct = age_performance.div(age_performance.sum(axis=1), axis=0) * 100\n",
    "\n",
    "age_performance_pct.plot(kind='bar', ax=axes[0,1], color=colors[:4])\n",
    "axes[0,1].set_title('Distribui√ß√£o de Desempenho por Faixa Et√°ria', fontweight='bold')\n",
    "axes[0,1].set_xlabel('Faixa Et√°ria')\n",
    "axes[0,1].set_ylabel('Percentual (%)')\n",
    "axes[0,1].legend(title='Desempenho', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "axes[0,1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 3. An√°lise de Educa√ß√£o dos Pais\n",
    "parent_education_performance = df_processed.groupby(['parental_education_level', 'performance_category']).size().unstack(fill_value=0)\n",
    "parent_education_performance_pct = parent_education_performance.div(parent_education_performance.sum(axis=1), axis=0) * 100\n",
    "\n",
    "parent_education_performance_pct.plot(kind='bar', ax=axes[1,0], color=colors[:4])\n",
    "axes[1,0].set_title('Distribui√ß√£o de Desempenho por Educa√ß√£o dos Pais', fontweight='bold')\n",
    "axes[1,0].set_xlabel('Educa√ß√£o dos Pais')\n",
    "axes[1,0].set_ylabel('Percentual (%)')\n",
    "axes[1,0].legend(title='Desempenho', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "axes[1,0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 4. An√°lise de Participa√ß√£o Extracurricular\n",
    "extracurricular_performance = df_processed.groupby(['extracurricular_participation', 'performance_category']).size().unstack(fill_value=0)\n",
    "extracurricular_performance_pct = extracurricular_performance.div(extracurricular_performance.sum(axis=1), axis=0) * 100\n",
    "\n",
    "extracurricular_performance_pct.plot(kind='bar', ax=axes[1,1], color=colors[:4])\n",
    "axes[1,1].set_title('Distribui√ß√£o de Desempenho por Participa√ß√£o Extracurricular', fontweight='bold')\n",
    "axes[1,1].set_xlabel('Participa√ß√£o Extracurricular')\n",
    "axes[1,1].set_ylabel('Percentual (%)')\n",
    "axes[1,1].legend(title='Desempenho', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "axes[1,1].tick_params(axis='x', rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e5b8f0",
   "metadata": {},
   "source": [
    "## üí° Insights e Recomenda√ß√µes\n",
    "\n",
    "Esta se√ß√£o apresenta as principais descobertas e sugest√µes baseadas na an√°lise:\n",
    "\n",
    "### üéØ Principais Descobertas:\n",
    "- **Fatores de Impacto** - Vari√°veis que mais influenciam o desempenho\n",
    "- **Perfis de Estudantes** - Segmenta√ß√£o por comportamentos e resultados\n",
    "- **Padr√µes de Risco** - Identifica√ß√£o de estudantes em situa√ß√£o vulner√°vel\n",
    "- **Oportunidades de Melhoria** - √Åreas com potencial de interven√ß√£o\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b458c082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 8. INSIGHTS E RECOMENDA√á√ïES \n",
    "# =============================================================================\n",
    "\n",
    "print(\"üí° INSIGHTS E RECOMENDA√á√ïES AVAN√áADAS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# =============================================================================\n",
    "# 8.1 AN√ÅLISE DE SEGMENTA√á√ÉO DE ESTUDANTES\n",
    "# =============================================================================\n",
    "print(\"üéØ SEGMENTA√á√ÉO DE ESTUDANTES POR PERFORMANCE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Definir segmentos baseados no desempenho\n",
    "def segment_students(score):\n",
    "    if score >= 85:\n",
    "        return \"Excelente\"\n",
    "    elif score >= 70:\n",
    "        return \"Bom\"\n",
    "    elif score >= 50:\n",
    "        return \"Regular\"\n",
    "    else:\n",
    "        return \"Baixo\"\n",
    "\n",
    "df_processed['performance_segment'] = df_processed['exam_score'].apply(segment_students)\n",
    "\n",
    "# An√°lise por segmento\n",
    "segment_analysis = df_processed.groupby('performance_segment').agg({\n",
    "    'study_hours_per_day': 'mean',\n",
    "    'social_media_hours': 'mean',\n",
    "    'sleep_hours': 'mean',\n",
    "    'mental_health_rating': 'mean',\n",
    "    'exercise_frequency': 'mean',\n",
    "    'attendance_percentage': 'mean',\n",
    "    'exam_score': ['count', 'mean', 'std']\n",
    "}).round(2)\n",
    "\n",
    "print(\"üìä CARACTER√çSTICAS POR SEGMENTO DE PERFORMANCE:\")\n",
    "print(\"-\" * 60)\n",
    "for segment in ['Excelente', 'Bom', 'Regular', 'Baixo']:\n",
    "    if segment in segment_analysis.index:\n",
    "        data = segment_analysis.loc[segment]\n",
    "        count = data[('exam_score', 'count')]\n",
    "        mean_score = data[('exam_score', 'mean')]\n",
    "        std_score = data[('exam_score', 'std')]\n",
    "        \n",
    "        print(f\"\\nüéØ SEGMENTO {segment.upper()}:\")\n",
    "        print(f\"  ‚Ä¢ Quantidade: {count} estudantes ({count/len(df_processed)*100:.1f}%)\")\n",
    "        print(f\"  ‚Ä¢ Nota m√©dia: {mean_score:.1f} ¬± {std_score:.1f}\")\n",
    "        print(f\"  ‚Ä¢ Horas de estudo: {data[('study_hours_per_day', 'mean')]:.1f}h/dia\")\n",
    "        print(f\"  ‚Ä¢ Redes sociais: {data[('social_media_hours', 'mean')]:.1f}h/dia\")\n",
    "        print(f\"  ‚Ä¢ Sono: {data[('sleep_hours', 'mean')]:.1f}h\")\n",
    "        print(f\"  ‚Ä¢ Sa√∫de mental: {data[('mental_health_rating', 'mean')]:.1f}/10\")\n",
    "        print(f\"  ‚Ä¢ Exerc√≠cio: {data[('exercise_frequency', 'mean')]:.1f}\")\n",
    "        print(f\"  ‚Ä¢ Presen√ßa: {data[('attendance_percentage', 'mean')]:.1f}%\")\n",
    "\n",
    "# =============================================================================\n",
    "# 8.2 AN√ÅLISE DE FATORES DE RISCO\n",
    "# =============================================================================\n",
    "print(\"\\n‚ö†Ô∏è  AN√ÅLISE DE FATORES DE RISCO\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Identificar estudantes em risco\n",
    "risk_factors = []\n",
    "risk_factors.extend(df_processed[df_processed['study_hours_per_day'] < 2].index.tolist())\n",
    "risk_factors.extend(df_processed[df_processed['mental_health_rating'] < 4].index.tolist())\n",
    "risk_factors.extend(df_processed[df_processed['attendance_percentage'] < 70].index.tolist())\n",
    "risk_factors.extend(df_processed[df_processed['social_media_hours'] > 6].index.tolist())\n",
    "\n",
    "# Contar fatores de risco por estudante\n",
    "risk_count = pd.Series(risk_factors).value_counts()\n",
    "high_risk_students = risk_count[risk_count >= 2].index\n",
    "\n",
    "print(f\"üö® Estudantes com m√∫ltiplos fatores de risco: {len(high_risk_students)}\")\n",
    "print(f\"üìä Distribui√ß√£o de fatores de risco:\")\n",
    "print(f\"  ‚Ä¢ 0 fatores: {len(df_processed) - len(risk_count)} estudantes\")\n",
    "print(f\"  ‚Ä¢ 1 fator: {len(risk_count[risk_count == 1])} estudantes\")\n",
    "print(f\"  ‚Ä¢ 2+ fatores: {len(high_risk_students)} estudantes\")\n",
    "\n",
    "# An√°lise dos estudantes de alto risco\n",
    "if len(high_risk_students) > 0:\n",
    "    high_risk_data = df_processed.loc[high_risk_students]\n",
    "    print(f\"\\nüîç CARACTER√çSTICAS DOS ESTUDANTES DE ALTO RISCO:\")\n",
    "    print(f\"  ‚Ä¢ Nota m√©dia: {high_risk_data['exam_score'].mean():.1f}\")\n",
    "    print(f\"  ‚Ä¢ Horas de estudo: {high_risk_data['study_hours_per_day'].mean():.1f}h/dia\")\n",
    "    print(f\"  ‚Ä¢ Sa√∫de mental: {high_risk_data['mental_health_rating'].mean():.1f}/10\")\n",
    "\n",
    "# =============================================================================\n",
    "# 8.3 RECOMENDA√á√ïES PERSONALIZADAS\n",
    "# =============================================================================\n",
    "print(\"\\nüí° RECOMENDA√á√ïES PERSONALIZADAS POR SEGMENTO\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "recommendations = {\n",
    "    'Excelente': [\n",
    "        \"üéØ Manter estrat√©gias atuais de estudo\",\n",
    "        \"üìö Explorar desafios acad√™micos adicionais\",\n",
    "        \"ü§ù Mentorear estudantes com dificuldades\",\n",
    "        \"üèÜ Participar de competi√ß√µes acad√™micas\"\n",
    "    ],\n",
    "    'Bom': [\n",
    "        \"üìà Aumentar horas de estudo em 1-2h/dia\",\n",
    "        \"üß† Melhorar t√©cnicas de estudo\",\n",
    "        \"‚è∞ Otimizar gest√£o de tempo\",\n",
    "        \"üéØ Definir metas espec√≠ficas de melhoria\"\n",
    "    ],\n",
    "    'Regular': [\n",
    "        \"üìö Implementar rotina de estudo estruturada\",\n",
    "        \"üßò Praticar t√©cnicas de relaxamento\",\n",
    "        \"üë• Buscar apoio acad√™mico\",\n",
    "        \"üì± Reduzir tempo em redes sociais\"\n",
    "    ],\n",
    "    'Baixo': [\n",
    "        \"üö® Interven√ß√£o imediata necess√°ria\",\n",
    "        \"üë®‚Äçüè´ Acompanhamento pedag√≥gico intensivo\",\n",
    "        \"üß† Avalia√ß√£o de sa√∫de mental\",\n",
    "        \"üìû Suporte familiar e institucional\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "for segment, recs in recommendations.items():\n",
    "    if segment in df_processed['performance_segment'].values:\n",
    "        count = len(df_processed[df_processed['performance_segment'] == segment])\n",
    "        print(f\"\\nüéØ SEGMENTO {segment.upper()} ({count} estudantes):\")\n",
    "        for rec in recs:\n",
    "            print(f\"  {rec}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 8.4 M√âTRICAS DE IMPACTO E ROI\n",
    "# =============================================================================\n",
    "print(\"\\nüìä M√âTRICAS DE IMPACTO E ROI\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Calcular impacto potencial das interven√ß√µes\n",
    "current_avg_score = df_processed['exam_score'].mean()\n",
    "excellent_avg_score = df_processed[df_processed['performance_segment'] == 'Excelente']['exam_score'].mean()\n",
    "\n",
    "# Simular impacto de melhorias\n",
    "improvement_scenarios = {\n",
    "    'Aumentar horas de estudo em 1h/dia': {\n",
    "        'target_segments': ['Regular', 'Baixo'],\n",
    "        'expected_improvement': 5,  # pontos\n",
    "        'cost': 'Baixo'\n",
    "    },\n",
    "    'Melhorar sa√∫de mental': {\n",
    "        'target_segments': ['Regular', 'Baixo'],\n",
    "        'expected_improvement': 8,  # pontos\n",
    "        'cost': 'M√©dio'\n",
    "    },\n",
    "    'Reduzir redes sociais em 2h/dia': {\n",
    "        'target_segments': ['Regular', 'Baixo'],\n",
    "        'expected_improvement': 6,  # pontos\n",
    "        'cost': 'Baixo'\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"üéØ CEN√ÅRIOS DE MELHORIA:\")\n",
    "print(\"-\" * 40)\n",
    "for intervention, details in improvement_scenarios.items():\n",
    "    target_count = sum([len(df_processed[df_processed['performance_segment'] == seg]) \n",
    "                       for seg in details['target_segments']])\n",
    "    potential_impact = target_count * details['expected_improvement']\n",
    "    \n",
    "    print(f\"\\nüìà {intervention}:\")\n",
    "    print(f\"  ‚Ä¢ Estudantes impactados: {target_count}\")\n",
    "    print(f\"  ‚Ä¢ Melhoria esperada: {details['expected_improvement']} pontos\")\n",
    "    print(f\"  ‚Ä¢ Impacto total: {potential_impact} pontos\")\n",
    "    print(f\"  ‚Ä¢ Custo: {details['cost']}\")\n",
    "\n",
    "print(f\"\\nüéØ M√âTRICAS ATUAIS:\")\n",
    "print(f\"  ‚Ä¢ Nota m√©dia atual: {current_avg_score:.1f}\")\n",
    "print(f\"  ‚Ä¢ Nota m√©dia dos excelentes: {excellent_avg_score:.1f}\")\n",
    "print(f\"  ‚Ä¢ Potencial de melhoria: {excellent_avg_score - current_avg_score:.1f} pontos\")\n",
    "\n",
    "print(\"\\n‚úÖ An√°lise de insights e recomenda√ß√µes conclu√≠da!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb46c015",
   "metadata": {},
   "source": [
    "## üîó An√°lise de Correla√ß√µes\n",
    "\n",
    "Esta se√ß√£o explora as rela√ß√µes entre diferentes vari√°veis:\n",
    "\n",
    "### üìà Tipos de Correla√ß√µes Analisadas:\n",
    "- **Correla√ß√µes Positivas** - Fatores que melhoram o desempenho\n",
    "- **Correla√ß√µes Negativas** - Fatores que prejudicam o desempenho\n",
    "- **Matriz de Correla√ß√£o** - Vis√£o completa das rela√ß√µes\n",
    "- **Fatores de Impacto** - Ranking das vari√°veis mais influentes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f7eaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 7. VISUALIZA√á√ïES DE CORRELA√á√ïES/ ANALISAR BEM ESSE AQ\n",
    "# =============================================================================\n",
    "\n",
    "print(\"üìà MAPA DE CALOR DE CORRELA√á√ïES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Criar figura com subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "# Criar matriz de correla√ß√£o excluindo a coluna 'cluster'\n",
    "numeric_cols = df_processed.select_dtypes(include=[np.number]).columns\n",
    "# Excluir a coluna 'cluster' da an√°lise de correla√ß√£o\n",
    "numeric_cols = numeric_cols.drop('cluster') if 'cluster' in numeric_cols else numeric_cols\n",
    "correlation_matrix = df_processed[numeric_cols].corr()\n",
    "\n",
    "# Mapa de calor completo\n",
    "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "sns.heatmap(correlation_matrix, \n",
    "            mask=mask,\n",
    "            annot=True, \n",
    "            cmap='RdYlBu_r', \n",
    "            center=0,\n",
    "            square=True,\n",
    "            fmt='.2f',\n",
    "            cbar_kws={\"shrink\": .8},\n",
    "            ax=ax1)\n",
    "ax1.set_title('Mapa de Calor: Correla√ß√µes Completas', fontsize=16, fontweight='bold', pad=20)\n",
    "\n",
    "# Mapa de calor focado no desempenho acad√™mico\n",
    "exam_corr_data = correlation_matrix[['exam_score']].sort_values('exam_score', ascending=False)\n",
    "sns.heatmap(exam_corr_data, \n",
    "            annot=True, \n",
    "            cmap='RdYlGn', \n",
    "            center=0,\n",
    "            fmt='.3f',\n",
    "            cbar_kws={\"shrink\": .8},\n",
    "            ax=ax2)\n",
    "ax2.set_title('Correla√ß√µes com Desempenho Acad√™mico', fontsize=16, fontweight='bold', pad=20)\n",
    "ax2.set_xlabel('')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92342c2f",
   "metadata": {},
   "source": [
    "## üìä An√°lises Comparativas Detalhadas\n",
    "\n",
    "Esta se√ß√£o realiza compara√ß√µes aprofundadas entre diferentes grupos:\n",
    "\n",
    "### üîç Compara√ß√µes Realizadas:\n",
    "- **Por G√™nero** - Diferen√ßas comportamentais entre homens e mulheres\n",
    "- **Por Educa√ß√£o dos Pais** - Impacto do background familiar\n",
    "- **Por Faixa Et√°ria** - Varia√ß√µes por idade\n",
    "- **Por N√≠vel de Desempenho** - Caracter√≠sticas de cada segmento\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021617c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 8. AN√ÅLISES COMPARATIVAS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"üìä AN√ÅLISE DETALHADA E INSIGHTS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# An√°lise de perfis de estudantes\n",
    "print(\"üéØ PERFIS DE ESTUDANTES POR DESEMPENHO\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Estudantes de alto desempenho (top 25%)\n",
    "high_performers = df_processed[df_processed['performance_category'] == 'Excelente']\n",
    "print(f\"üìà Estudantes de Excelente Desempenho: {len(high_performers)} ({len(high_performers)/len(df_processed)*100:.1f}%)\")\n",
    "\n",
    "print(\"\\nüîç CARACTER√çSTICAS DOS ESTUDANTES DE ALTO DESEMPENHO:\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"‚Ä¢ M√©dia de horas de estudo: {high_performers['study_hours_per_day'].mean():.1f} horas/dia\")\n",
    "print(f\"‚Ä¢ M√©dia de horas de redes sociais: {high_performers['social_media_hours'].mean():.1f} horas/dia\")\n",
    "print(f\"‚Ä¢ M√©dia de horas de sono: {high_performers['sleep_hours'].mean():.1f} horas\")\n",
    "print(f\"‚Ä¢ M√©dia de frequ√™ncia de exerc√≠cio: {high_performers['exercise_frequency'].mean():.1f}\")\n",
    "print(f\"‚Ä¢ M√©dia de sa√∫de mental: {high_performers['mental_health_rating'].mean():.1f}/10\")\n",
    "print(f\"‚Ä¢ Participa√ß√£o em atividades extracurriculares: {high_performers['extracurricular_participation'].value_counts(normalize=True)['Yes']*100:.1f}%\")\n",
    "\n",
    "# Estudantes de baixo desempenho\n",
    "low_performers = df_processed[df_processed['performance_category'] == 'Baixo']\n",
    "print(f\"\\nüìâ Estudantes de Baixo Desempenho: {len(low_performers)} ({len(low_performers)/len(df_processed)*100:.1f}%)\")\n",
    "\n",
    "print(\"\\nüîç CARACTER√çSTICAS DOS ESTUDANTES DE BAIXO DESEMPENHO:\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"‚Ä¢ M√©dia de horas de estudo: {low_performers['study_hours_per_day'].mean():.1f} horas/dia\")\n",
    "print(f\"‚Ä¢ M√©dia de horas de redes sociais: {low_performers['social_media_hours'].mean():.1f} horas/dia\")\n",
    "print(f\"‚Ä¢ M√©dia de horas de sono: {low_performers['sleep_hours'].mean():.1f} horas\")\n",
    "print(f\"‚Ä¢ M√©dia de frequ√™ncia de exerc√≠cio: {low_performers['exercise_frequency'].mean():.1f}\")\n",
    "print(f\"‚Ä¢ M√©dia de sa√∫de mental: {low_performers['mental_health_rating'].mean():.1f}/10\")\n",
    "print(f\"‚Ä¢ Participa√ß√£o em atividades extracurriculares: {low_performers['extracurricular_participation'].value_counts(normalize=True)['Yes']*100:.1f}%\")\n",
    "\n",
    "# An√°lise de correla√ß√µes mais significativas\n",
    "print(\"\\nüéØ PRINCIPAIS INSIGHTS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Criar matriz de correla√ß√£o excluindo a coluna 'cluster'\n",
    "numeric_cols = df_processed.select_dtypes(include=[np.number]).columns\n",
    "# Excluir a coluna 'cluster' da an√°lise de correla√ß√£o\n",
    "numeric_cols = numeric_cols.drop('cluster') if 'cluster' in numeric_cols else numeric_cols\n",
    "correlation_matrix = df_processed[numeric_cols].corr()\n",
    "exam_correlations = correlation_matrix['exam_score'].drop('exam_score').sort_values(ascending=True)\n",
    "\n",
    "# Top 3 vari√°veis com maior impacto positivo\n",
    "top_3_positive = exam_correlations[exam_correlations.index != 'exam_score'].head(3)\n",
    "print(\"üìà TOP 3 FATORES POSITIVOS:\")\n",
    "for i, (var, corr) in enumerate(top_3_positive.items(), 1):\n",
    "    print(f\"{i}. {var}: {corr:.3f}\")\n",
    "\n",
    "# Top 3 vari√°veis com maior impacto negativo\n",
    "top_3_negative = exam_correlations[exam_correlations < 0].tail(3)\n",
    "print(\"\\nüìâ TOP 3 FATORES NEGATIVOS:\")\n",
    "for i, (var, corr) in enumerate(top_3_negative.items(), 1):\n",
    "    print(f\"{i}. {var}: {corr:.3f}\")\n",
    "\n",
    "# An√°lise de diferen√ßas por g√™nero\n",
    "print(\"\\nüë• AN√ÅLISE POR G√äNERO\")\n",
    "print(\"=\" * 30)\n",
    "gender_analysis = df_processed.groupby('gender')['exam_score'].agg(['count', 'mean', 'std'])\n",
    "display(gender_analysis.round(2))\n",
    "\n",
    "# An√°lise de diferen√ßas por educa√ß√£o dos pais\n",
    "print(\"\\nüë®‚Äçüë©‚Äçüëß‚Äçüë¶ AN√ÅLISE POR EDUCA√á√ÉO DOS PAIS\")\n",
    "print(\"=\" * 40)\n",
    "parent_education_analysis = df_processed.groupby('parental_education_level')['exam_score'].agg(['count', 'mean', 'std'])\n",
    "display(parent_education_analysis.round(2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0ecf18",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Dashboard:\n",
    "- **M√©tricas-Chave** - Indicadores principais de desempenho\n",
    "- **Distribui√ß√£o de Performance** - Segmenta√ß√£o dos estudantes\n",
    "- **Fatores de Impacto** - Vari√°veis mais influentes\n",
    "- **An√°lise de Clusters** - Agrupamento por comportamentos similares\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a376f191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 10. AN√ÅLISE DAS TEND√äNCIAS IDENTIFICADAS E SEGMENTA√á√ÉO DE ESTUDANTES\n",
    "# =============================================================================\n",
    "\n",
    "print(\"üìä AN√ÅLISE DAS TEND√äNCIAS E SEGMENTA√á√ÉO DE ESTUDANTES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Configura√ß√µes para visualiza√ß√µes\n",
    "plt.style.use('default')\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams.update({\n",
    "    'figure.figsize': (20, 15),\n",
    "    'font.size': 12,\n",
    "    'axes.titlesize': 16,\n",
    "    'axes.labelsize': 14,\n",
    "    'xtick.labelsize': 12,\n",
    "    'ytick.labelsize': 12,\n",
    "    'legend.fontsize': 12\n",
    "})\n",
    "\n",
    "# Paleta de cores\n",
    "colors = ['#2E86AB', '#A23B72', '#F18F01', '#C73E1D', '#7209B7', '#F77F00', '#06FFA5']\n",
    "\n",
    "# =============================================================================\n",
    "# 10.1 TEND√äNCIA 1: Exam score tem alta correla√ß√£o com study hours per day\n",
    "# =============================================================================\n",
    "\n",
    "print(\"üìà TEND√äNCIA 1: Correla√ß√£o Alta entre Horas de Estudo e Desempenho\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(20, 15))\n",
    "\n",
    "# Gr√°fico 1: Scatter plot com linha de regress√£o\n",
    "correlation_study = df_processed['study_hours_per_day'].corr(df_processed['exam_score'])\n",
    "ax1.scatter(df_processed['study_hours_per_day'], df_processed['exam_score'], \n",
    "           alpha=0.6, c=df_processed['mental_health_rating'], cmap='viridis', s=50)\n",
    "ax1.set_xlabel('Horas de Estudo por Dia')\n",
    "ax1.set_ylabel('Nota do Exame')\n",
    "ax1.set_title(f'Rela√ß√£o Horas de Estudo vs Desempenho\\nCorrela√ß√£o: {correlation_study:.3f}', \n",
    "              fontweight='bold', fontsize=14)\n",
    "\n",
    "# Linha de regress√£o\n",
    "z = np.polyfit(df_processed['study_hours_per_day'], df_processed['exam_score'], 1)\n",
    "p = np.poly1d(z)\n",
    "ax1.plot(df_processed['study_hours_per_day'], p(df_processed['study_hours_per_day']), \n",
    "         \"r--\", alpha=0.8, linewidth=3)\n",
    "\n",
    "# Segmenta√ß√£o por horas de estudo\n",
    "def categorize_study_intensity(hours):\n",
    "    if hours <= 2: return 'Baixo (‚â§2h)'\n",
    "    elif hours <= 4: return 'Moderado (2-4h)'\n",
    "    elif hours <= 6: return 'Alto (4-6h)'\n",
    "    else: return 'Muito Alto (>6h)'\n",
    "\n",
    "df_processed['study_intensity'] = df_processed['study_hours_per_day'].apply(categorize_study_intensity)\n",
    "\n",
    "# Gr√°fico 2: Boxplot por intensidade de estudo\n",
    "study_groups = df_processed.groupby('study_intensity')['exam_score'].apply(list)\n",
    "study_labels = list(study_groups.index)\n",
    "study_data = list(study_groups.values)\n",
    "\n",
    "box_plot = ax2.boxplot(study_data, labels=study_labels, patch_artist=True)\n",
    "for patch, color in zip(box_plot['boxes'], colors[:len(study_labels)]):\n",
    "    patch.set_facecolor(color)\n",
    "    patch.set_alpha(0.7)\n",
    "\n",
    "ax2.set_ylabel('Nota do Exame')\n",
    "ax2.set_title('Desempenho por Intensidade de Estudo', fontweight='bold', fontsize=14)\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# An√°lise estat√≠stica por grupo\n",
    "print(\"\\nüìä AN√ÅLISE POR GRUPOS DE INTENSIDADE DE ESTUDO:\")\n",
    "print(\"-\" * 60)\n",
    "for group in study_labels:\n",
    "    group_data = df_processed[df_processed['study_intensity'] == group]\n",
    "    print(f\"üéØ {group}:\")\n",
    "    print(f\"  ‚Ä¢ Quantidade: {len(group_data)} estudantes ({len(group_data)/len(df_processed)*100:.1f}%)\")\n",
    "    print(f\"  ‚Ä¢ Nota m√©dia: {group_data['exam_score'].mean():.1f} ¬± {group_data['exam_score'].std():.1f}\")\n",
    "    print(f\"  ‚Ä¢ Horas m√©dias de estudo: {group_data['study_hours_per_day'].mean():.1f}h/dia\")\n",
    "\n",
    "# =============================================================================\n",
    "# 10.2 TEND√äNCIA 2: Exam score √© negativamente afetado por uso redes sociais e Netflix\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\nüìâ TEND√äNCIA 2: Impacto Negativo das Redes Sociais e Netflix\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Gr√°fico 3: Scatter plot redes sociais vs desempenho\n",
    "correlation_social = df_processed['social_media_hours'].corr(df_processed['exam_score'])\n",
    "ax3.scatter(df_processed['social_media_hours'], df_processed['exam_score'], \n",
    "           alpha=0.6, c=df_processed['study_hours_per_day'], cmap='plasma', s=50)\n",
    "ax3.set_xlabel('Horas de Redes Sociais por Dia')\n",
    "ax3.set_ylabel('Nota do Exame')\n",
    "ax3.set_title(f'Rela√ß√£o Redes Sociais vs Desempenho\\nCorrela√ß√£o: {correlation_social:.3f}', \n",
    "              fontweight='bold', fontsize=14)\n",
    "\n",
    "# Linha de regress√£o\n",
    "z_social = np.polyfit(df_processed['social_media_hours'], df_processed['exam_score'], 1)\n",
    "p_social = np.poly1d(z_social)\n",
    "ax3.plot(df_processed['social_media_hours'], p_social(df_processed['social_media_hours']), \n",
    "         \"r--\", alpha=0.8, linewidth=3)\n",
    "\n",
    "# Segmenta√ß√£o por uso de redes sociais\n",
    "def categorize_social_usage(hours):\n",
    "    if hours <= 1: return 'Baixo (‚â§1h)'\n",
    "    elif hours <= 3: return 'Moderado (1-3h)'\n",
    "    elif hours <= 5: return 'Alto (3-5h)'\n",
    "    else: return 'Muito Alto (>5h)'\n",
    "\n",
    "df_processed['social_usage'] = df_processed['social_media_hours'].apply(categorize_social_usage)\n",
    "\n",
    "# Gr√°fico 4: Boxplot por uso de redes sociais\n",
    "social_groups = df_processed.groupby('social_usage')['exam_score'].apply(list)\n",
    "social_labels = list(social_groups.index)\n",
    "social_data = list(social_groups.values)\n",
    "\n",
    "box_plot_social = ax4.boxplot(social_data, labels=social_labels, patch_artist=True)\n",
    "for patch, color in zip(box_plot_social['boxes'], colors[:len(social_labels)]):\n",
    "    patch.set_facecolor(color)\n",
    "    patch.set_alpha(0.7)\n",
    "\n",
    "ax4.set_ylabel('Nota do Exame')\n",
    "ax4.set_title('Desempenho por Uso de Redes Sociais', fontweight='bold', fontsize=14)\n",
    "ax4.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# An√°lise estat√≠stica por grupo de redes sociais\n",
    "print(\"\\nüìä AN√ÅLISE POR GRUPOS DE USO DE REDES SOCIAIS:\")\n",
    "print(\"-\" * 60)\n",
    "for group in social_labels:\n",
    "    group_data = df_processed[df_processed['social_usage'] == group]\n",
    "    print(f\"üéØ {group}:\")\n",
    "    print(f\"  ‚Ä¢ Quantidade: {len(group_data)} estudantes ({len(group_data)/len(df_processed)*100:.1f}%)\")\n",
    "    print(f\"  ‚Ä¢ Nota m√©dia: {group_data['exam_score'].mean():.1f} ¬± {group_data['exam_score'].std():.1f}\")\n",
    "    print(f\"  ‚Ä¢ Horas m√©dias de redes sociais: {group_data['social_media_hours'].mean():.1f}h/dia\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# =============================================================================\n",
    "# 10.3 TEND√äNCIA 3: Exam score √© levemente afetado por boas horas de sono e exerc√≠cios\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\nüò¥ TEND√äNCIA 3: Impacto do Sono e Exerc√≠cios no Desempenho\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(20, 15))\n",
    "\n",
    "# Gr√°fico 1: Sono vs Desempenho\n",
    "correlation_sleep = df_processed['sleep_hours'].corr(df_processed['exam_score'])\n",
    "ax1.scatter(df_processed['sleep_hours'], df_processed['exam_score'], \n",
    "           alpha=0.6, c=df_processed['mental_health_rating'], cmap='coolwarm', s=50)\n",
    "ax1.set_xlabel('Horas de Sono por Noite')\n",
    "ax1.set_ylabel('Nota do Exame')\n",
    "ax1.set_title(f'Rela√ß√£o Sono vs Desempenho\\nCorrela√ß√£o: {correlation_sleep:.3f}', \n",
    "              fontweight='bold', fontsize=14)\n",
    "\n",
    "# Linha de regress√£o\n",
    "z_sleep = np.polyfit(df_processed['sleep_hours'], df_processed['exam_score'], 1)\n",
    "p_sleep = np.poly1d(z_sleep)\n",
    "ax1.plot(df_processed['sleep_hours'], p_sleep(df_processed['sleep_hours']), \n",
    "         \"r--\", alpha=0.8, linewidth=3)\n",
    "\n",
    "# Segmenta√ß√£o por qualidade do sono\n",
    "def categorize_sleep_quality(hours):\n",
    "    if hours < 6: return 'Insuficiente (<6h)'\n",
    "    elif hours <= 7: return 'Adequado (6-7h)'\n",
    "    elif hours <= 9: return 'Ideal (7-9h)'\n",
    "    else: return 'Excessivo (>9h)'\n",
    "\n",
    "df_processed['sleep_quality'] = df_processed['sleep_hours'].apply(categorize_sleep_quality)\n",
    "\n",
    "# Gr√°fico 2: Boxplot por qualidade do sono\n",
    "sleep_groups = df_processed.groupby('sleep_quality')['exam_score'].apply(list)\n",
    "sleep_labels = list(sleep_groups.index)\n",
    "sleep_data = list(sleep_groups.values)\n",
    "\n",
    "box_plot_sleep = ax2.boxplot(sleep_data, labels=sleep_labels, patch_artist=True)\n",
    "for patch, color in zip(box_plot_sleep['boxes'], colors[:len(sleep_labels)]):\n",
    "    patch.set_facecolor(color)\n",
    "    patch.set_alpha(0.7)\n",
    "\n",
    "ax2.set_ylabel('Nota do Exame')\n",
    "ax2.set_title('Desempenho por Qualidade do Sono', fontweight='bold', fontsize=14)\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Gr√°fico 3: Exerc√≠cios vs Desempenho\n",
    "correlation_exercise = df_processed['exercise_frequency'].corr(df_processed['exam_score'])\n",
    "ax3.scatter(df_processed['exercise_frequency'], df_processed['exam_score'], \n",
    "           alpha=0.6, c=df_processed['sleep_hours'], cmap='spring', s=50)\n",
    "ax3.set_xlabel('Frequ√™ncia de Exerc√≠cios (vezes/semana)')\n",
    "ax3.set_ylabel('Nota do Exame')\n",
    "ax3.set_title(f'Rela√ß√£o Exerc√≠cios vs Desempenho\\nCorrela√ß√£o: {correlation_exercise:.3f}', \n",
    "              fontweight='bold', fontsize=14)\n",
    "\n",
    "# Linha de regress√£o\n",
    "z_exercise = np.polyfit(df_processed['exercise_frequency'], df_processed['exam_score'], 1)\n",
    "p_exercise = np.poly1d(z_exercise)\n",
    "ax3.plot(df_processed['exercise_frequency'], p_exercise(df_processed['exercise_frequency']), \n",
    "         \"r--\", alpha=0.8, linewidth=3)\n",
    "\n",
    "# Segmenta√ß√£o por frequ√™ncia de exerc√≠cios\n",
    "def categorize_exercise_frequency(freq):\n",
    "    if freq <= 1: return 'Sedent√°rio (‚â§1x)'\n",
    "    elif freq <= 3: return 'Ocasional (1-3x)'\n",
    "    elif freq <= 5: return 'Regular (3-5x)'\n",
    "    else: return 'Ativo (>5x)'\n",
    "\n",
    "df_processed['exercise_level'] = df_processed['exercise_frequency'].apply(categorize_exercise_frequency)\n",
    "\n",
    "# Gr√°fico 4: Boxplot por n√≠vel de exerc√≠cio\n",
    "exercise_groups = df_processed.groupby('exercise_level')['exam_score'].apply(list)\n",
    "exercise_labels = list(exercise_groups.index)\n",
    "exercise_data = list(exercise_groups.values)\n",
    "\n",
    "box_plot_exercise = ax4.boxplot(exercise_data, labels=exercise_labels, patch_artist=True)\n",
    "for patch, color in zip(box_plot_exercise['boxes'], colors[:len(exercise_labels)]):\n",
    "    patch.set_facecolor(color)\n",
    "    patch.set_alpha(0.7)\n",
    "\n",
    "ax4.set_ylabel('Nota do Exame')\n",
    "ax4.set_title('Desempenho por N√≠vel de Exerc√≠cio', fontweight='bold', fontsize=14)\n",
    "ax4.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# An√°lise estat√≠stica por grupos de sono e exerc√≠cio\n",
    "print(\"\\nüìä AN√ÅLISE POR GRUPOS DE SONO:\")\n",
    "print(\"-\" * 50)\n",
    "for group in sleep_labels:\n",
    "    group_data = df_processed[df_processed['sleep_quality'] == group]\n",
    "    print(f\"üò¥ {group}:\")\n",
    "    print(f\"  ‚Ä¢ Quantidade: {len(group_data)} estudantes ({len(group_data)/len(df_processed)*100:.1f}%)\")\n",
    "    print(f\"  ‚Ä¢ Nota m√©dia: {group_data['exam_score'].mean():.1f} ¬± {group_data['exam_score'].std():.1f}\")\n",
    "    print(f\"  ‚Ä¢ Horas m√©dias de sono: {group_data['sleep_hours'].mean():.1f}h\")\n",
    "\n",
    "print(\"\\nüìä AN√ÅLISE POR GRUPOS DE EXERC√çCIO:\")\n",
    "print(\"-\" * 50)\n",
    "for group in exercise_labels:\n",
    "    group_data = df_processed[df_processed['exercise_level'] == group]\n",
    "    print(f\"üèÉ {group}:\")\n",
    "    print(f\"  ‚Ä¢ Quantidade: {len(group_data)} estudantes ({len(group_data)/len(df_processed)*100:.1f}%)\")\n",
    "    print(f\"  ‚Ä¢ Nota m√©dia: {group_data['exam_score'].mean():.1f} ¬± {group_data['exam_score'].std():.1f}\")\n",
    "    print(f\"  ‚Ä¢ Frequ√™ncia m√©dia de exerc√≠cio: {group_data['exercise_frequency'].mean():.1f}x/semana\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# =============================================================================\n",
    "# 10.4 TEND√äNCIA 4: Exam Score tem rela√ß√£o direta com study_screen_ratio\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\nüì± TEND√äNCIA 4: Rela√ß√£o com Raz√£o Estudo/Tela\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(20, 15))\n",
    "\n",
    "# Gr√°fico 1: Raz√£o estudo/tela vs desempenho\n",
    "correlation_ratio = df_processed['study_screen_ratio'].corr(df_processed['exam_score'])\n",
    "ax1.scatter(df_processed['study_screen_ratio'], df_processed['exam_score'], \n",
    "           alpha=0.6, c=df_processed['mental_health_rating'], cmap='viridis', s=50)\n",
    "ax1.set_xlabel('Raz√£o Estudo/Tela (study_screen_ratio)')\n",
    "ax1.set_ylabel('Nota do Exame')\n",
    "ax1.set_title(f'Rela√ß√£o Raz√£o Estudo/Tela vs Desempenho\\nCorrela√ß√£o: {correlation_ratio:.3f}', \n",
    "              fontweight='bold', fontsize=14)\n",
    "\n",
    "# Linha de regress√£o\n",
    "z_ratio = np.polyfit(df_processed['study_screen_ratio'], df_processed['exam_score'], 1)\n",
    "p_ratio = np.poly1d(z_ratio)\n",
    "ax1.plot(df_processed['study_screen_ratio'], p_ratio(df_processed['study_screen_ratio']), \n",
    "         \"r--\", alpha=0.8, linewidth=3)\n",
    "\n",
    "# Segmenta√ß√£o por raz√£o estudo/tela\n",
    "def categorize_study_screen_ratio(ratio):\n",
    "    if ratio <= 0.5: return 'Baixa (‚â§0.5)'\n",
    "    elif ratio <= 1.0: return 'Moderada (0.5-1.0)'\n",
    "    elif ratio <= 2.0: return 'Alta (1.0-2.0)'\n",
    "    else: return 'Muito Alta (>2.0)'\n",
    "\n",
    "df_processed['study_screen_level'] = df_processed['study_screen_ratio'].apply(categorize_study_screen_ratio)\n",
    "\n",
    "# Gr√°fico 2: Boxplot por n√≠vel de raz√£o estudo/tela\n",
    "ratio_groups = df_processed.groupby('study_screen_level')['exam_score'].apply(list)\n",
    "ratio_labels = list(ratio_groups.index)\n",
    "ratio_data = list(ratio_groups.values)\n",
    "\n",
    "box_plot_ratio = ax2.boxplot(ratio_data, labels=ratio_labels, patch_artist=True)\n",
    "for patch, color in zip(box_plot_ratio['boxes'], colors[:len(ratio_labels)]):\n",
    "    patch.set_facecolor(color)\n",
    "    patch.set_alpha(0.7)\n",
    "\n",
    "ax2.set_ylabel('Nota do Exame')\n",
    "ax2.set_title('Desempenho por N√≠vel de Raz√£o Estudo/Tela', fontweight='bold', fontsize=14)\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Gr√°fico 3: An√°lise combinada - Estudo vs Redes Sociais\n",
    "ax3.scatter(df_processed['study_hours_per_day'], df_processed['social_media_hours'], \n",
    "           c=df_processed['exam_score'], cmap='RdYlGn', s=60, alpha=0.7)\n",
    "ax3.set_xlabel('Horas de Estudo por Dia')\n",
    "ax3.set_ylabel('Horas de Redes Sociais por Dia')\n",
    "ax3.set_title('Estudo vs Redes Sociais (Cor: Desempenho)', fontweight='bold', fontsize=14)\n",
    "\n",
    "# Adicionar contornos\n",
    "from scipy.stats import gaussian_kde\n",
    "xy = np.vstack([df_processed['study_hours_per_day'], df_processed['social_media_hours']])\n",
    "density = gaussian_kde(xy)\n",
    "x_range = np.linspace(df_processed['study_hours_per_day'].min(), df_processed['study_hours_per_day'].max(), 50)\n",
    "y_range = np.linspace(df_processed['social_media_hours'].min(), df_processed['social_media_hours'].max(), 50)\n",
    "X, Y = np.meshgrid(x_range, y_range)\n",
    "positions = np.vstack([X.ravel(), Y.ravel()])\n",
    "Z = np.reshape(density(positions).T, X.shape)\n",
    "ax3.contour(X, Y, Z, levels=5, colors='black', alpha=0.6, linewidths=1)\n",
    "\n",
    "# Gr√°fico 4: An√°lise de perfis combinados\n",
    "# Criar perfis baseados em m√∫ltiplas vari√°veis\n",
    "def create_student_profile(row):\n",
    "    study_hours = row['study_hours_per_day']\n",
    "    social_hours = row['social_media_hours']\n",
    "    sleep_hours = row['sleep_hours']\n",
    "    exercise_freq = row['exercise_frequency']\n",
    "    \n",
    "    if study_hours >= 5 and social_hours <= 2 and sleep_hours >= 7:\n",
    "        return 'Estudante Ideal'\n",
    "    elif study_hours >= 4 and social_hours <= 3:\n",
    "        return 'Estudante Dedicado'\n",
    "    elif study_hours >= 3 and social_hours <= 4:\n",
    "        return 'Estudante Moderado'\n",
    "    elif social_hours >= 5:\n",
    "        return 'Estudante Distra√≠do'\n",
    "    else:\n",
    "        return 'Estudante Irregular'\n",
    "\n",
    "df_processed['student_profile'] = df_processed.apply(create_student_profile, axis=1)\n",
    "\n",
    "# Contar perfis\n",
    "profile_counts = df_processed['student_profile'].value_counts()\n",
    "profile_performance = df_processed.groupby('student_profile')['exam_score'].mean().sort_values(ascending=False)\n",
    "\n",
    "# Gr√°fico de barras dos perfis\n",
    "bars = ax4.bar(range(len(profile_counts)), profile_counts.values, \n",
    "               color=colors[:len(profile_counts)], alpha=0.7)\n",
    "ax4.set_xlabel('Perfil do Estudante')\n",
    "ax4.set_ylabel('N√∫mero de Estudantes')\n",
    "ax4.set_title('Distribui√ß√£o por Perfil de Estudante', fontweight='bold', fontsize=14)\n",
    "ax4.set_xticks(range(len(profile_counts)))\n",
    "ax4.set_xticklabels(profile_counts.index, rotation=45, ha='right')\n",
    "\n",
    "# Adicionar valores de performance nas barras\n",
    "for i, (bar, perf) in enumerate(zip(bars, profile_performance.values)):\n",
    "    ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 5, \n",
    "             f'{perf:.1f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# An√°lise estat√≠stica por grupos de raz√£o estudo/tela\n",
    "print(\"\\nüìä AN√ÅLISE POR GRUPOS DE RAZ√ÉO ESTUDO/TELA:\")\n",
    "print(\"-\" * 60)\n",
    "for group in ratio_labels:\n",
    "    group_data = df_processed[df_processed['study_screen_level'] == group]\n",
    "    print(f\"üì± {group}:\")\n",
    "    print(f\"  ‚Ä¢ Quantidade: {len(group_data)} estudantes ({len(group_data)/len(df_processed)*100:.1f}%)\")\n",
    "    print(f\"  ‚Ä¢ Nota m√©dia: {group_data['exam_score'].mean():.1f} ¬± {group_data['exam_score'].std():.1f}\")\n",
    "    print(f\"  ‚Ä¢ Raz√£o m√©dia: {group_data['study_screen_ratio'].mean():.2f}\")\n",
    "\n",
    "# An√°lise por perfis de estudantes\n",
    "print(\"\\nüìä AN√ÅLISE POR PERFIS DE ESTUDANTES:\")\n",
    "print(\"-\" * 50)\n",
    "for profile in profile_performance.index:\n",
    "    profile_data = df_processed[df_processed['student_profile'] == profile]\n",
    "    print(f\"üë§ {profile}:\")\n",
    "    print(f\"  ‚Ä¢ Quantidade: {len(profile_data)} estudantes ({len(profile_data)/len(df_processed)*100:.1f}%)\")\n",
    "    print(f\"  ‚Ä¢ Nota m√©dia: {profile_data['exam_score'].mean():.1f} ¬± {profile_data['exam_score'].std():.1f}\")\n",
    "    print(f\"  ‚Ä¢ Horas de estudo: {profile_data['study_hours_per_day'].mean():.1f}h/dia\")\n",
    "    print(f\"  ‚Ä¢ Horas de redes sociais: {profile_data['social_media_hours'].mean():.1f}h/dia\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# =============================================================================\n",
    "# 10.5 TEND√äNCIA 5: G√™nero tem rela√ß√£o mas bem baixa, mulheres tem nota um pouquinho mais alto\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\nüë• TEND√äNCIA 5: An√°lise por G√™nero\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(20, 15))\n",
    "\n",
    "# Gr√°fico 1: Distribui√ß√£o de notas por g√™nero\n",
    "gender_performance = df_processed.groupby('gender')['exam_score'].apply(list)\n",
    "gender_labels = list(gender_performance.index)\n",
    "gender_data = list(gender_performance.values)\n",
    "\n",
    "box_plot_gender = ax1.boxplot(gender_data, labels=gender_labels, patch_artist=True)\n",
    "for patch, color in zip(box_plot_gender['boxes'], colors[:len(gender_labels)]):\n",
    "    patch.set_facecolor(color)\n",
    "    patch.set_alpha(0.7)\n",
    "\n",
    "ax1.set_ylabel('Nota do Exame')\n",
    "ax1.set_title('Distribui√ß√£o de Notas por G√™nero', fontweight='bold', fontsize=14)\n",
    "\n",
    "# An√°lise estat√≠stica por g√™nero\n",
    "print(\"\\nüìä AN√ÅLISE POR G√äNERO:\")\n",
    "print(\"-\" * 40)\n",
    "for gender in gender_labels:\n",
    "    gender_data = df_processed[df_processed['gender'] == gender]\n",
    "    print(f\"üë§ {gender}:\")\n",
    "    print(f\"  ‚Ä¢ Quantidade: {len(gender_data)} estudantes ({len(gender_data)/len(df_processed)*100:.1f}%)\")\n",
    "    print(f\"  ‚Ä¢ Nota m√©dia: {gender_data['exam_score'].mean():.1f} ¬± {gender_data['exam_score'].std():.1f}\")\n",
    "    print(f\"  ‚Ä¢ Horas de estudo: {gender_data['study_hours_per_day'].mean():.1f}h/dia\")\n",
    "    print(f\"  ‚Ä¢ Horas de redes sociais: {gender_data['social_media_hours'].mean():.1f}h/dia\")\n",
    "    print(f\"  ‚Ä¢ Sa√∫de mental: {gender_data['mental_health_rating'].mean():.1f}/10\")\n",
    "\n",
    "# Gr√°fico 2: An√°lise de correla√ß√µes por g√™nero\n",
    "correlations_by_gender = {}\n",
    "for gender in gender_labels:\n",
    "    gender_data = df_processed[df_processed['gender'] == gender]\n",
    "    correlations_by_gender[gender] = {\n",
    "        'study_hours': gender_data['study_hours_per_day'].corr(gender_data['exam_score']),\n",
    "        'social_media': gender_data['social_media_hours'].corr(gender_data['exam_score']),\n",
    "        'sleep': gender_data['sleep_hours'].corr(gender_data['exam_score']),\n",
    "        'exercise': gender_data['exercise_frequency'].corr(gender_data['exam_score'])\n",
    "    }\n",
    "\n",
    "# Gr√°fico de barras das correla√ß√µes por g√™nero\n",
    "correlation_vars = ['study_hours', 'social_media', 'sleep', 'exercise']\n",
    "x = np.arange(len(correlation_vars))\n",
    "width = 0.35\n",
    "\n",
    "for i, gender in enumerate(gender_labels):\n",
    "    correlations = [correlations_by_gender[gender][var] for var in correlation_vars]\n",
    "    ax2.bar(x + i*width, correlations, width, label=gender, alpha=0.7)\n",
    "\n",
    "ax2.set_xlabel('Vari√°veis')\n",
    "ax2.set_ylabel('Correla√ß√£o com Desempenho')\n",
    "ax2.set_title('Correla√ß√µes por G√™nero', fontweight='bold', fontsize=14)\n",
    "ax2.set_xticks(x + width/2)\n",
    "ax2.set_xticklabels([var.replace('_', ' ').title() for var in correlation_vars])\n",
    "ax2.legend()\n",
    "ax2.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "\n",
    "# Gr√°fico 3: An√°lise de distribui√ß√£o de performance por g√™nero\n",
    "performance_by_gender = pd.crosstab(df_processed['gender'], df_processed['performance_category'])\n",
    "performance_by_gender_pct = performance_by_gender.div(performance_by_gender.sum(axis=1), axis=0) * 100\n",
    "\n",
    "performance_by_gender_pct.plot(kind='bar', ax=ax3, color=colors[:4])\n",
    "ax3.set_title('Distribui√ß√£o de Performance por G√™nero', fontweight='bold', fontsize=14)\n",
    "ax3.set_xlabel('G√™nero')\n",
    "ax3.set_ylabel('Percentual (%)')\n",
    "ax3.legend(title='Performance', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "ax3.tick_params(axis='x', rotation=0)\n",
    "\n",
    "# Gr√°fico 4: An√°lise de h√°bitos por g√™nero\n",
    "habits_by_gender = df_processed.groupby('gender')[['study_hours_per_day', 'social_media_hours', \n",
    "                                                   'sleep_hours', 'exercise_frequency']].mean()\n",
    "\n",
    "habits_by_gender.plot(kind='bar', ax=ax4, color=colors[:4])\n",
    "ax4.set_title('H√°bitos por G√™nero', fontweight='bold', fontsize=14)\n",
    "ax4.set_xlabel('G√™nero')\n",
    "ax4.set_ylabel('Valor M√©dio')\n",
    "ax4.legend(title='H√°bitos', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "ax4.tick_params(axis='x', rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# =============================================================================\n",
    "# 10.6 RESUMO DAS TEND√äNCIAS E SEGMENTA√á√ÉO FINAL\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\nüéØ RESUMO DAS TEND√äNCIAS E SEGMENTA√á√ÉO FINAL\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Criar segmenta√ß√£o final baseada em m√∫ltiplas vari√°veis\n",
    "def create_final_segments(row):\n",
    "    score = row['exam_score']\n",
    "    study_hours = row['study_hours_per_day']\n",
    "    social_hours = row['social_media_hours']\n",
    "    sleep_hours = row['sleep_hours']\n",
    "    exercise_freq = row['exercise_frequency']\n",
    "    mental_health = row['mental_health_rating']\n",
    "    \n",
    "    # Segmenta√ß√£o baseada em performance e h√°bitos\n",
    "    if score >= 85 and study_hours >= 4 and social_hours <= 3:\n",
    "        return 'Estrelas Acad√™micas'\n",
    "    elif score >= 70 and study_hours >= 3 and social_hours <= 4:\n",
    "        return 'Estudantes Dedicados'\n",
    "    elif score >= 50 and study_hours >= 2:\n",
    "        return 'Estudantes em Desenvolvimento'\n",
    "    elif social_hours >= 5 or study_hours < 2:\n",
    "        return 'Estudantes em Risco'\n",
    "    else:\n",
    "        return 'Estudantes Regulares'\n",
    "\n",
    "df_processed['final_segment'] = df_processed.apply(create_final_segments, axis=1)\n",
    "\n",
    "# An√°lise dos segmentos finais\n",
    "print(\"\\nüìä SEGMENTA√á√ÉO FINAL DOS ESTUDANTES:\")\n",
    "print(\"-\" * 60)\n",
    "segment_analysis = df_processed.groupby('final_segment').agg({\n",
    "    'exam_score': ['count', 'mean', 'std'],\n",
    "    'study_hours_per_day': 'mean',\n",
    "    'social_media_hours': 'mean',\n",
    "    'sleep_hours': 'mean',\n",
    "    'exercise_frequency': 'mean',\n",
    "    'mental_health_rating': 'mean'\n",
    "}).round(2)\n",
    "\n",
    "for segment in segment_analysis.index:\n",
    "    data = segment_analysis.loc[segment]\n",
    "    count = data[('exam_score', 'count')]\n",
    "    mean_score = data[('exam_score', 'mean')]\n",
    "    std_score = data[('exam_score', 'std')]\n",
    "    \n",
    "    print(f\"\\nüéØ {segment.upper()}:\")\n",
    "    print(f\"  ‚Ä¢ Quantidade: {count} estudantes ({count/len(df_processed)*100:.1f}%)\")\n",
    "    print(f\"  ‚Ä¢ Nota m√©dia: {mean_score:.1f} ¬± {std_score:.1f}\")\n",
    "    print(f\"  ‚Ä¢ Horas de estudo: {data[('study_hours_per_day', 'mean')]:.1f}h/dia\")\n",
    "    print(f\"  ‚Ä¢ Horas de redes sociais: {data[('social_media_hours', 'mean')]:.1f}h/dia\")\n",
    "    print(f\"  ‚Ä¢ Horas de sono: {data[('sleep_hours', 'mean')]:.1f}h\")\n",
    "    print(f\"  ‚Ä¢ Frequ√™ncia de exerc√≠cio: {data[('exercise_frequency', 'mean')]:.1f}x/semana\")\n",
    "    print(f\"  ‚Ä¢ Sa√∫de mental: {data[('mental_health_rating', 'mean')]:.1f}/10\")\n",
    "\n",
    "# Correla√ß√µes finais\n",
    "print(f\"\\nüìà CORRELA√á√ïES FINAIS DAS TEND√äNCIAS:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"1. Horas de Estudo vs Desempenho: {df_processed['study_hours_per_day'].corr(df_processed['exam_score']):.3f}\")\n",
    "print(f\"2. Redes Sociais vs Desempenho: {df_processed['social_media_hours'].corr(df_processed['exam_score']):.3f}\")\n",
    "print(f\"3. Netflix vs Desempenho: {df_processed['netflix_hours'].corr(df_processed['exam_score']):.3f}\")\n",
    "print(f\"4. Sono vs Desempenho: {df_processed['sleep_hours'].corr(df_processed['exam_score']):.3f}\")\n",
    "print(f\"5. Exerc√≠cio vs Desempenho: {df_processed['exercise_frequency'].corr(df_processed['exam_score']):.3f}\")\n",
    "print(f\"6. Raz√£o Estudo/Tela vs Desempenho: {df_processed['study_screen_ratio'].corr(df_processed['exam_score']):.3f}\")\n",
    "\n",
    "# An√°lise por g√™nero\n",
    "gender_correlation = df_processed.groupby('gender')['exam_score'].mean()\n",
    "print(f\"\\nüë• DIFEREN√áA POR G√äNERO:\")\n",
    "print(\"-\" * 30)\n",
    "for gender in gender_correlation.index:\n",
    "    print(f\"  ‚Ä¢ {gender}: {gender_correlation[gender]:.1f} pontos\")\n",
    "print(f\"  ‚Ä¢ Diferen√ßa: {gender_correlation.max() - gender_correlation.min():.1f} pontos\")\n",
    "\n",
    "print(f\"\\n‚úÖ AN√ÅLISE DAS TEND√äNCIAS CONCLU√çDA!\")\n",
    "print(f\"üìä Total de segmentos criados: {len(df_processed['final_segment'].unique())}\")\n",
    "print(f\"üéØ Estudantes analisados: {len(df_processed)}\")\n",
    "print(f\"üìà Vari√°veis analisadas: {df_processed.shape[1]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f9e9e8",
   "metadata": {},
   "source": [
    "## üìà An√°lise de Tend√™ncias e Segmenta√ß√£o\n",
    "\n",
    "Esta se√ß√£o identifica padr√µes comportamentais e segmenta os estudantes:\n",
    "\n",
    "### üéØ An√°lises Realizadas:\n",
    "- **Segmenta√ß√£o por Performance** - Agrupamento por n√≠veis de desempenho\n",
    "- **Identifica√ß√£o de Padr√µes** - Comportamentos comuns em cada grupo\n",
    "- **An√°lise de Risco** - Estudantes em situa√ß√£o vulner√°vel\n",
    "- **Recomenda√ß√µes Personalizadas** - Sugest√µes espec√≠ficas por segmento\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cee7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# HEATMAP COMPLETO - TODAS AS VARI√ÅVEIS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"üìä HEATMAP COMPLETO - TODAS AS VARI√ÅVEIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Configura√ß√µes para visualiza√ß√£o\n",
    "plt.style.use('default')\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams.update({\n",
    "    'figure.figsize': (20, 16),\n",
    "    'font.size': 12,\n",
    "    'axes.titlesize': 18,\n",
    "    'axes.labelsize': 14,\n",
    "    'xtick.labelsize': 10,\n",
    "    'ytick.labelsize': 10,\n",
    "    'legend.fontsize': 12\n",
    "})\n",
    "\n",
    "# Selecionar todas as vari√°veis num√©ricas do dataset original + novas vari√°veis\n",
    "numeric_vars_original = [\n",
    "    'age', 'study_hours_per_day', 'social_media_hours', 'netflix_hours',\n",
    "    'attendance_percentage', 'sleep_hours', 'exercise_frequency', \n",
    "    'mental_health_rating', 'exam_score'\n",
    "]\n",
    "\n",
    "# Adicionar as 4 novas vari√°veis criadas\n",
    "new_vars = ['total_leisure_hours', 'available_study_time', 'study_screen_ratio', 'wellness_index']\n",
    "\n",
    "# Vari√°veis categ√≥ricas que ser√£o transformadas\n",
    "categorical_vars = [\n",
    "    'gender', 'part_time_job', 'diet_quality', 'parental_education_level',\n",
    "    'internet_quality', 'extracurricular_participation'\n",
    "]\n",
    "\n",
    "# Combinar todas as vari√°veis\n",
    "all_numeric_vars = numeric_vars_original + new_vars\n",
    "\n",
    "# Criar DataFrame com todas as vari√°veis num√©ricas\n",
    "df_heatmap = df_processed[all_numeric_vars].copy()\n",
    "\n",
    "# =============================================================================\n",
    "# TRANSFORMA√á√ÉO DE VARI√ÅVEIS CATEG√ìRICAS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"üîÑ TRANSFORMANDO VARI√ÅVEIS CATEG√ìRICAS EM NUM√âRICAS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Transformar vari√°veis categ√≥ricas em num√©ricas\n",
    "for var in categorical_vars:\n",
    "    if var in df_processed.columns:\n",
    "        print(f\"üìä Transformando {var}...\")\n",
    "        \n",
    "        # Usar LabelEncoder para transformar categ√≥ricas em num√©ricas\n",
    "        from sklearn.preprocessing import LabelEncoder\n",
    "        le = LabelEncoder()\n",
    "        \n",
    "        # Aplicar transforma√ß√£o\n",
    "        df_heatmap[f'{var}_encoded'] = le.fit_transform(df_processed[var].astype(str))\n",
    "        \n",
    "        # Mostrar mapeamento\n",
    "        unique_values = df_processed[var].unique()\n",
    "        encoded_values = le.transform(unique_values.astype(str))\n",
    "        mapping = dict(zip(unique_values, encoded_values))\n",
    "        print(f\"   Mapeamento: {mapping}\")\n",
    "\n",
    "# Atualizar lista de vari√°veis para incluir as transformadas\n",
    "encoded_vars = [f'{var}_encoded' for var in categorical_vars if var in df_processed.columns]\n",
    "all_vars_for_heatmap = all_numeric_vars + encoded_vars\n",
    "\n",
    "print(f\"\\n‚úÖ Transforma√ß√£o conclu√≠da!\")\n",
    "print(f\"üìä Total de vari√°veis no heatmap: {len(all_vars_for_heatmap)}\")\n",
    "print(f\"   ‚Ä¢ Vari√°veis num√©ricas originais: {len(numeric_vars_original)}\")\n",
    "print(f\"   ‚Ä¢ Novas vari√°veis criadas: {len(new_vars)}\")\n",
    "print(f\"   ‚Ä¢ Vari√°veis categ√≥ricas transformadas: {len(encoded_vars)}\")\n",
    "\n",
    "# Calcular matriz de correla√ß√£o com todas as vari√°veis\n",
    "correlation_matrix_complete = df_heatmap[all_vars_for_heatmap].corr()\n",
    "\n",
    "# Criar figura com subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 10))\n",
    "\n",
    "# =============================================================================\n",
    "# HEATMAP 1: MATRIZ COMPLETA COM M√ÅSCARA\n",
    "# =============================================================================\n",
    "\n",
    "# Criar m√°scara para mostrar apenas metade da matriz (evitar duplica√ß√£o)\n",
    "mask = np.triu(np.ones_like(correlation_matrix_complete, dtype=bool))\n",
    "\n",
    "# Heatmap completo\n",
    "sns.heatmap(correlation_matrix_complete, \n",
    "            mask=mask,\n",
    "            annot=True, \n",
    "            cmap='RdBu_r', \n",
    "            center=0,\n",
    "            square=True,\n",
    "            fmt='.2f',\n",
    "            cbar_kws={\"shrink\": .8},\n",
    "            ax=ax1,\n",
    "            linewidths=0.5,\n",
    "            annot_kws={'size': 8})\n",
    "\n",
    "ax1.set_title('Matriz de Correla√ß√µes Completa\\n(Todas as Vari√°veis)', \n",
    "              fontsize=16, fontweight='bold', pad=20)\n",
    "\n",
    "# =============================================================================\n",
    "# HEATMAP 2: FOCO NO EXAM_SCORE\n",
    "# =============================================================================\n",
    "\n",
    "# Selecionar apenas correla√ß√µes com exam_score\n",
    "exam_correlations = correlation_matrix_complete['exam_score'].drop('exam_score').sort_values(ascending=False)\n",
    "\n",
    "# Criar DataFrame para o heatmap focado\n",
    "exam_corr_df = pd.DataFrame({\n",
    "    'exam_score': exam_correlations\n",
    "}).T\n",
    "\n",
    "# Heatmap focado no desempenho\n",
    "sns.heatmap(exam_corr_df, \n",
    "            annot=True, \n",
    "            cmap='RdYlGn', \n",
    "            center=0,\n",
    "            fmt='.3f',\n",
    "            cbar_kws={\"shrink\": .8},\n",
    "            ax=ax2,\n",
    "            linewidths=0.5,\n",
    "            annot_kws={'size': 10})\n",
    "\n",
    "ax2.set_title('Correla√ß√µes com Desempenho Acad√™mico\\n(Exam Score)', \n",
    "              fontsize=16, fontweight='bold', pad=20)\n",
    "ax2.set_xlabel('')\n",
    "\n",
    "# Ajustar layout\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# =============================================================================\n",
    "# AN√ÅLISE DETALHADA DAS CORRELA√á√ïES\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\nüìà AN√ÅLISE DETALHADA DAS CORRELA√á√ïES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Separar correla√ß√µes por tipo\n",
    "positive_correlations = exam_correlations[exam_correlations > 0].sort_values(ascending=False)\n",
    "negative_correlations = exam_correlations[exam_correlations < 0].sort_values(ascending=True)\n",
    "\n",
    "print(\"\\nüü¢ CORRELA√á√ïES POSITIVAS (Fatores que Melhoram o Desempenho):\")\n",
    "print(\"-\" * 70)\n",
    "for var, corr in positive_correlations.items():\n",
    "    strength = \"Muito Forte\" if abs(corr) > 0.7 else \"Forte\" if abs(corr) > 0.5 else \"Moderada\" if abs(corr) > 0.3 else \"Fraca\"\n",
    "    print(f\"üìà {var:25} | {corr:6.3f} | {strength}\")\n",
    "\n",
    "print(\"\\nüî¥ CORRELA√á√ïES NEGATIVAS (Fatores que Reduzem o Desempenho):\")\n",
    "print(\"-\" * 70)\n",
    "for var, corr in negative_correlations.items():\n",
    "    strength = \"Muito Forte\" if abs(corr) > 0.7 else \"Forte\" if abs(corr) > 0.5 else \"Moderada\" if abs(corr) > 0.3 else \"Fraca\"\n",
    "    print(f\"üìâ {var:25} | {corr:6.3f} | {strength}\")\n",
    "\n",
    "# =============================================================================\n",
    "# AN√ÅLISE DAS NOVAS VARI√ÅVEIS E CATEG√ìRICAS TRANSFORMADAS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\nüÜï AN√ÅLISE DAS NOVAS VARI√ÅVEIS CRIADAS:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "new_vars_correlations = exam_correlations[new_vars]\n",
    "for var in new_vars:\n",
    "    corr = new_vars_correlations[var]\n",
    "    strength = \"Muito Forte\" if abs(corr) > 0.7 else \"Forte\" if abs(corr) > 0.5 else \"Moderada\" if abs(corr) > 0.3 else \"Fraca\"\n",
    "    direction = \"üìà\" if corr > 0 else \"üìâ\"\n",
    "    print(f\"{direction} {var:25} | {corr:6.3f} | {strength}\")\n",
    "\n",
    "print(\"\\nüè∑Ô∏è AN√ÅLISE DAS VARI√ÅVEIS CATEG√ìRICAS TRANSFORMADAS:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "encoded_vars_correlations = exam_correlations[encoded_vars]\n",
    "for var in encoded_vars:\n",
    "    corr = encoded_vars_correlations[var]\n",
    "    strength = \"Muito Forte\" if abs(corr) > 0.7 else \"Forte\" if abs(corr) > 0.5 else \"Moderada\" if abs(corr) > 0.3 else \"Fraca\"\n",
    "    direction = \"üìà\" if corr > 0 else \"üìâ\"\n",
    "    original_var = var.replace('_encoded', '')\n",
    "    print(f\"{direction} {var:25} | {corr:6.3f} | {strength} ({original_var})\")\n",
    "\n",
    "# =============================================================================\n",
    "# RANKING DE IMPORT√ÇNCIA\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\nüèÜ RANKING DE IMPORT√ÇNCIA DAS VARI√ÅVEIS:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Criar ranking por valor absoluto da correla√ß√£o\n",
    "importance_ranking = exam_correlations.abs().sort_values(ascending=False)\n",
    "\n",
    "for i, (var, abs_corr) in enumerate(importance_ranking.items(), 1):\n",
    "    original_corr = exam_correlations[var]\n",
    "    direction = \"üìà\" if original_corr > 0 else \"üìâ\"\n",
    "    strength = \"Muito Forte\" if abs_corr > 0.7 else \"Forte\" if abs_corr > 0.5 else \"Moderada\" if abs_corr > 0.3 else \"Fraca\"\n",
    "    print(f\"{i:2d}. {direction} {var:25} | {original_corr:6.3f} | {strength}\")\n",
    "\n",
    "# =============================================================================\n",
    "# ESTAT√çSTICAS RESUMIDAS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\nüìä ESTAT√çSTICAS RESUMIDAS:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"‚Ä¢ Total de vari√°veis analisadas: {len(all_vars_for_heatmap)}\")\n",
    "print(f\"‚Ä¢ Vari√°veis num√©ricas originais: {len(numeric_vars_original)}\")\n",
    "print(f\"‚Ä¢ Novas vari√°veis criadas: {len(new_vars)}\")\n",
    "print(f\"‚Ä¢ Vari√°veis categ√≥ricas transformadas: {len(encoded_vars)}\")\n",
    "print(f\"‚Ä¢ Correla√ß√µes positivas: {len(positive_correlations)}\")\n",
    "print(f\"‚Ä¢ Correla√ß√µes negativas: {len(negative_correlations)}\")\n",
    "print(f\"‚Ä¢ Correla√ß√£o mais forte: {exam_correlations.abs().max():.3f}\")\n",
    "print(f\"‚Ä¢ Vari√°vel mais importante: {exam_correlations.abs().idxmax()}\")\n",
    "\n",
    "# =============================================================================\n",
    "# INSIGHTS PRINCIPAIS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\nüí° INSIGHTS PRINCIPAIS DO HEATMAP:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "top_3_positive = positive_correlations.head(3)\n",
    "top_3_negative = negative_correlations.head(3)\n",
    "\n",
    "print(\"üü¢ TOP 3 FATORES POSITIVOS:\")\n",
    "for i, (var, corr) in enumerate(top_3_positive.items(), 1):\n",
    "    print(f\"  {i}. {var}: {corr:.3f}\")\n",
    "\n",
    "print(\"\\nüî¥ TOP 3 FATORES NEGATIVOS:\")\n",
    "for i, (var, corr) in enumerate(top_3_negative.items(), 1):\n",
    "    print(f\"  {i}. {var}: {corr:.3f}\")\n",
    "\n",
    "print(\"\\nüÜï IMPACTO DAS NOVAS VARI√ÅVEIS:\")\n",
    "for var in new_vars:\n",
    "    corr = exam_correlations[var]\n",
    "    rank = list(importance_ranking.index).index(var) + 1\n",
    "    print(f\"  ‚Ä¢ {var}: {corr:.3f} (Rank #{rank})\")\n",
    "\n",
    "print(\"\\nüè∑Ô∏è IMPACTO DAS VARI√ÅVEIS CATEG√ìRICAS TRANSFORMADAS:\")\n",
    "for var in encoded_vars:\n",
    "    corr = exam_correlations[var]\n",
    "    rank = list(importance_ranking.index).index(var) + 1\n",
    "    original_var = var.replace('_encoded', '')\n",
    "    print(f\"  ‚Ä¢ {original_var}: {corr:.3f} (Rank #{rank})\")\n",
    "\n",
    "print(f\"\\n‚úÖ HEATMAP COMPLETO GERADO COM SUCESSO!\")\n",
    "print(f\"üìä An√°lise de {len(all_vars_for_heatmap)} vari√°veis conclu√≠da!\")\n",
    "print(f\"   ‚Ä¢ {len(numeric_vars_original)} num√©ricas originais\")\n",
    "print(f\"   ‚Ä¢ {len(new_vars)} novas vari√°veis criadas\") \n",
    "print(f\"   ‚Ä¢ {len(encoded_vars)} categ√≥ricas transformadas\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
